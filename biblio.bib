@inproceedings{amos2017input,
	title        = {Input convex neural networks},
	author       = {Amos, Brandon and Xu, Lei and Kolter, J Zico},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages        = {146--155},
	organization = {JMLR.org}
}
@article{amos2020svg,
	title        = {On the model-based stochastic value gradient for continuous reinforcement learning},
	author       = {Brandon Amos and Samuel Stanton and Denis Yarats and Andrew Gordon Wilson},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2008.12775},
	url          = {https://arxiv.org/abs/2008.12775},
	archiveprefix = {arXiv},
	eprint       = {2008.12775},
	timestamp    = {Wed, 16 Sep 2020 11:20:03 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2008-12775.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@techreport{baird1993counterexample,
	title        = {Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems},
	author       = {Williams, Ronald J and Baird III, Leemon C},
	year         = 1993,
	institution  = {Citeseer}
}
@inproceedings{blocher2017learning,
	title        = {Learning stable dynamical systems using contraction theory},
	author       = {Blocher, Caroline and Saveriano, Matteo and Lee, Dongheui},
	year         = 2017,
	booktitle    = {2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},
	pages        = {124--129},
	organization = {IEEE}
}
@inproceedings{boots2008constraint,
	title        = {A constraint generation approach to learning stable linear dynamical systems},
	author       = {Boots, Byron and Gordon, Geoffrey J and Siddiqi, Sajid M},
	year         = 2008,
	booktitle    = {Advances in neural information processing systems},
	pages        = {1329--1336}
}
@misc{brax2021github,
	title        = {Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
	author       = {C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
	year         = 2021,
	url          = {http://github.com/google/brax},
	version      = {0.0.15}
}
@article{buckman2018steve,
	title        = {Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion},
	author       = {Jacob Buckman and Danijar Hafner and George Tucker and Eugene Brevdo and Honglak Lee},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1807.01675},
	url          = {http://arxiv.org/abs/1807.01675},
	archiveprefix = {arXiv},
	eprint       = {1807.01675},
	timestamp    = {Mon, 13 Aug 2018 16:49:01 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1807-01675.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{burer2003nonlinear,
	title        = {A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
	author       = {Burer, Samuel and Monteiro, Renato DC},
	year         = 2003,
	journal      = {Mathematical Programming},
	publisher    = {Springer},
	volume       = 95,
	number       = 2,
	pages        = {329--357}
}
@inproceedings{chaudhuri2022first,
	title        = {First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach},
	author       = {Wagenmaker, Andrew J and Chen, Yifang and Simchowitz, Max and Du, Simon and Jamieson, Kevin},
	year         = 2022,
	month        = {Jul},
	booktitle    = {Proceedings of the 39th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 162,
	pages        = {22384--22429},
	url          = {https://proceedings.mlr.press/v162/wagenmaker22a.html},
	editor       = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	pdf          = {https://proceedings.mlr.press/v162/wagenmaker22a/wagenmaker22a.pdf}
}
@article{chen2018optimal,
	title        = {Optimal Control Via Neural Networks: A Convex Approach},
	author       = {Chen, Yize and Shi, Yuanyuan and Zhang, Baosen},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.11835}
}
@inproceedings{chen2019information,
	title        = {Information-Theoretic Considerations in Batch Reinforcement Learning},
	author       = {Chen, Jinglin and Jiang, Nan},
	year         = 2019,
	month        = {09--15 Jun},
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {1042--1051},
	url          = {https://proceedings.mlr.press/v97/chen19e.html},
	editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	pdf          = {http://proceedings.mlr.press/v97/chen19e/chen19e.pdf},
	abstract     = {Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.}
}
@inproceedings{chow2018lyapunov,
	title        = {A lyapunov-based approach to safe reinforcement learning},
	author       = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {8092--8101}
}
@inproceedings{chua2018pets,
	title        = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
	author       = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	year         = 2018,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf},
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}
@article{degrave2022magnetic,
	title        = {Magnetic control of tokamak plasmas through deep reinforcement learning},
	author       = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de Las Casas, Diego and others},
	year         = 2022,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 602,
	number       = 7897,
	pages        = {414--419}
}
@article{diddigi2019convergent,
	title        = {A convergent off-policy temporal difference algorithm},
	author       = {Diddigi, Raghuram Bharadwaj and Kamanchi, Chandramouli and Bhatnagar, Shalabh},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.05697}
}
@inproceedings{du2017stochastic,
	title        = {Stochastic variance reduction methods for policy evaluation},
	author       = {Du, Simon S and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1049--1058},
	organization = {PMLR}
}
@inproceedings{fedus2020revisiting,
	title        = {Revisiting fundamentals of experience replay},
	author       = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3061--3071},
	organization = {PMLR}
}
@article{Feinberg2018ModelBasedVE,
	title        = {Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning},
	author       = {Vladimir Feinberg and Alvin Wan and I. Stoica and Michael I. Jordan and Joseph E. Gonzalez and Sergey Levine},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1803.00101}
}
@article{fu2020d4rl,
	title        = {D4RL: Datasets for deep data-driven reinforcement learning},
	author       = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.07219}
}
@inproceedings{fujimoto2019off,
	title        = {Off-policy deep reinforcement learning without exploration},
	author       = {Fujimoto, Scott and Meger, David and Precup, Doina},
	year         = 2019,
	booktitle    = {International conference on machine learning},
	pages        = {2052--2062},
	organization = {PMLR}
}
@inproceedings{gal2016improving,
	title        = {Improving PILCO with Bayesian neural network dynamics models},
	author       = {Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
	year         = 2016,
	booktitle    = {Data-Efficient Machine Learning workshop, ICML},
	volume       = 4
}
@inproceedings{gelada2019off,
	title        = {Off-policy deep reinforcement learning by bootstrapping the covariate shift},
	author       = {Gelada, Carles and Bellemare, Marc G},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {3647--3655}
}
@inproceedings{gu2016continuous,
	title        = {Continuous deep q-learning with model-based acceleration},
	author       = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
	year         = 2016,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2829--2838}
}
@inproceedings{haarnoja2018soft,
	title        = {Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author       = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year         = 2018,
	booktitle    = {International conference on machine learning},
	pages        = {1861--1870},
	organization = {PMLR}
}
@inproceedings{hallak2017consistent,
	title        = {Consistent on-line off-policy evaluation},
	author       = {Hallak, Assaf and Mannor, Shie},
	year         = 2017,
	booktitle    = {International Conference on Machine Learning},
	pages        = {1372--1383},
	organization = {PMLR}
}
@article{hasselt2021expected,
	title        = {Expected eligibility traces},
	author       = {van Hasselt, Hado and Madjiheurem, Sephora and Hessel, Matteo and Silver, David and Barreto, Andr{\'e} and Borsa, Diana},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2007.01839}
}
@inproceedings{he2015delving,
	title        = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2015,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {1026--1034}
}
@inproceedings{janner2019mbpo,
	title        = {When to Trust Your Model: Model-Based Policy Optimization},
	author       = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@inproceedings{jiang2021emphatic,
	title        = {Emphatic Algorithms for Deep Reinforcement Learning},
	author       = {Jiang, Ray and Zahavy, Tom and Xu, Zhongwen and White, Adam and Hessel, Matteo and Blundell, Charles and Van Hasselt, Hado},
	year         = 2021,
	month        = 7,
	booktitle    = {Proceedings of the 38th International Conference on Machine Learning},
	publisher    = {PMLR},
	series       = {Proceedings of Machine Learning Research},
	volume       = 139,
	pages        = {5023--5033},
	url          = {https://proceedings.mlr.press/v139/jiang21j.html},
	editor       = {Meila, Marina and Zhang, Tong},
	pdf          = {http://proceedings.mlr.press/v139/jiang21j/jiang21j.pdf}
}
@article{jiang2021learning,
	title        = {Learning Expected Emphatic Traces for Deep RL},
	author       = {Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and van Hasselt, Hado},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.05405}
}
@article{jiang2022learning,
	title        = {Learning Expected Emphatic Traces for Deep RL},
	author       = {Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and Hasselt, Hado van},
	year         = 2022,
	month        = 6,
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 36,
	number       = 6,
	pages        = {7015--7023},
	doi          = {10.1609/aaai.v36i6.20660},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20660},
	abstractnote = {Off-policy sampling and experience replay are key for improving sample efficiency and scaling model-free temporal difference learning methods. When combined with function approximation, such as neural networks, this combination is known as the deadly triad and is potentially unstable. Recently, it has been shown that stability and good performance at scale can be achieved by combining emphatic weightings and multi-step updates. This approach, however, is generally limited to sampling complete trajectories in order, to compute the required emphatic weighting. In this paper we investigate how to combine emphatic weightings with non-sequential, off-line data sampled from a replay buffer. We develop a multi-step emphatic weighting that can be combined with replay, and a time-reversed n-step TD learning algorithm to learn the required emphatic weighting. We show that these state weightings reduce variance compared with prior approaches, while providing convergence guarantees. We tested the approach at scale on Atari 2600 video games, and observed that the new X-ETD(n) agent improved over baseline agents, highlighting both the scalability and broad applicability of our approach.}
}
@book{khalil2002nonlinear,
	title        = {Nonlinear systems},
	author       = {Khalil, Hassan K and Grizzle, Jessy W},
	year         = 2002,
	publisher    = {Prentice hall Upper Saddle River, NJ},
	volume       = 3
}
@article{khansari2011learning,
	title        = {Learning stable nonlinear dynamical systems with gaussian mixture models},
	author       = {Khansari-Zadeh, S Mohammad and Billard, Aude},
	year         = 2011,
	journal      = {IEEE Transactions on Robotics},
	publisher    = {IEEE},
	volume       = 27,
	number       = 5,
	pages        = {943--957}
}
@article{kidambi2020morel,
	title        = {Morel: Model-based offline reinforcement learning},
	author       = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {21810--21823}
}
@article{kingma2013auto,
	title        = {Auto-encoding variational bayes},
	author       = {Kingma, Diederik P and Welling, Max},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.6114}
}
@inproceedings{Kingma2014,
	title        = {{Auto-Encoding Variational Bayes}},
	author       = {Kingma, Diederik P. and Welling, Max},
	year         = 2014,
	booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	added-at     = {2020-10-15T14:36:56.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
	eprint       = {http://arxiv.org/abs/1312.6114v10},
	eprintclass  = {stat.ML},
	eprinttype   = {arXiv},
	file         = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
	interhash    = {a626a9d77a123c52405a08da983203cb},
	intrahash    = {42e5be6faa01cba2587f4907ac99dce8},
	keywords     = {cs.LG stat.ML vae},
	timestamp    = {2021-02-01T17:13:18.000+0100}
}
@article{kolter2011fixed,
	title        = {The fixed points of off-policy TD},
	author       = {Kolter, J},
	year         = 2011,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 24,
	pages        = {2169--2177}
}
@misc{kolternote2022,
	author       = {Kolter, Zico},
	year         = 2022,
	month        = 3,
	howpublished = {private communication}
}
@article{kumar19bear,
	title        = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
	author       = {Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
	url          = {http://arxiv.org/abs/1906.00949},
	conference   = {NeurIPS 2019}
}
@article{kumar2019stabilizing,
	title        = {Stabilizing off-policy q-learning via bootstrapping error reduction},
	author       = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@inproceedings{kumar2020cql,
	title        = {Conservative Q-Learning for Offline Reinforcement Learning},
	author       = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	url          = {https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html},
	editor       = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
	timestamp    = {Tue, 19 Jan 2021 15:57:33 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/KumarZTL20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kumar2020discor,
	title        = {Discor: Corrective feedback in reinforcement learning via distribution correction},
	author       = {Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2003.07305}
}
@inproceedings{kumar2022dr,
	title        = {{DR}3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization},
	author       = {Aviral Kumar and Rishabh Agarwal and Tengyu Ma and Aaron Courville and George Tucker and Sergey Levine},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=POvMvLi91f}
}
@book{la2012stability,
	title        = {Stability by Liapunov's Direct Method with Applications by Joseph L Salle and Solomon Lefschetz},
	author       = {La Salle, Joseph and Lefschetz, Solomon},
	year         = 2012,
	publisher    = {Elsevier},
	volume       = 4
}
@inproceedings{laroche2019safe,
	title        = {Safe policy improvement with baseline bootstrapping},
	author       = {Laroche, Romain and Trichelair, Paul and Des Combes, Remi Tachet},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {3652--3661},
	organization = {PMLR}
}
@inproceedings{lee2021sunrise,
	title        = {Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
	author       = {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {6131--6141},
	organization = {PMLR}
}
@article{levine2020offline,
	title        = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
	author       = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2005.01643},
	url          = {https://arxiv.org/abs/2005.01643},
	eprinttype   = {arXiv},
	eprint       = {2005.01643},
	timestamp    = {Fri, 08 May 2020 15:04:04 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2005-01643.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{levine2020survey,
	title        = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
	author       = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2005.01643},
	url          = {https://arxiv.org/abs/2005.01643},
	eprinttype   = {arXiv},
	eprint       = {2005.01643},
	timestamp    = {Fri, 08 May 2020 15:04:04 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2005-01643.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{liu2018breaking,
	title        = {Breaking the curse of horizon: Infinite-horizon off-policy estimation},
	author       = {Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
	year         = 2018,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31
}
@article{mahadevan2014proximal,
	title        = {Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces},
	author       = {Mahadevan, Sridhar and Liu, Bo and Thomas, Philip and Dabney, Will and Giguere, Steve and Jacek, Nicholas and Gemp, Ian and Liu, Ji},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1405.6757}
}
@article{manek2019stable,
	title        = {Learning stable deep dynamics models},
	author       = {Manek, Gaurav and Kolter, J Zico},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@article{manek2020stablevalue,
	title        = {Notes on Provably Stable Value Functions},
	author       = {Manek, Gaurav and Kolter, Zico},
	year         = 2020,
	month        = {Oct.},
	journal      = {unpublished}
}
@inproceedings{manek2022pitfalls,
	title        = {The Pitfalls of Regularization in Off-Policy {TD} Learning},
	author       = {Gaurav Manek and J Zico Kolter},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=vK53GLZJes8},
	editor       = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho}
}
@inproceedings{manek2023poptd,
	title        = {Projected Off-Policy TD Learning Stabilize Offline Reinforcement Learning},
	author       = {Gaurav Manek and Melrose Roderick and J Zico Kolter},
	year         = 2023,
	month        = {Jan.},
	article      = {in submission}
}
@inproceedings{mishra2017prediction,
	title        = {Prediction and control with temporal segment models},
	author       = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages        = {2459--2468},
	organization = {JMLR. org}
}
@article{mnih2013playing,
	title        = {Playing atari with deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.5602}
}
@article{mnih2015humanlevel,
	title        = {Human-level control through deep reinforcement learning},
	author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	year         = 2015,
	month        = feb,
	journal      = {Nature},
	publisher    = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	volume       = 518,
	number       = 7540,
	pages        = {529--533},
	issn         = {00280836},
	url          = {http://dx.doi.org/10.1038/nature14236},
	added-at     = {2015-08-26T14:46:40.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
	description  = {Human-level control through deep reinforcement learning - nature14236.pdf},
	interhash    = {eac59980357d99db87b341b61ef6645f},
	intrahash    = {fb15f4471c81dc2b9edf2304cb2f7083},
	keywords     = {deep learning toread},
	timestamp    = {2015-08-26T14:46:40.000+0200}
}
@article{nachum2019algaedice,
	title        = {Algaedice: Policy gradient from arbitrary experience},
	author       = {Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.02074}
}
@article{nachum2019dualdice,
	title        = {Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
	author       = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@inproceedings{nagabandi2018neural,
	title        = {Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
	author       = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
	year         = 2018,
	booktitle    = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
	pages        = {7559--7566},
	organization = {IEEE}
}
@misc{openaigym,
	title        = {OpenAI Gym},
	author       = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	year         = 2016,
	eprint       = {arXiv:1606.01540}
}
@inproceedings{papachristodoulou2002construction,
	title        = {On the construction of Lyapunov functions using the sum of squares decomposition},
	author       = {Papachristodoulou, Antonis and Prajna, Stephen},
	year         = 2002,
	booktitle    = {Proceedings of the 41st IEEE Conference on Decision and Control, 2002.},
	volume       = 3,
	pages        = {3482--3487},
	organization = {IEEE}
}
@phdthesis{parrilo2000structured,
	title        = {Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization},
	author       = {Parrilo, Pablo A},
	year         = 2000,
	school       = {California Institute of Technology}
}
@article{peng2019advantage,
	title        = {Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
	author       = {Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1910.00177}
}
@article{precup2000eligibility,
	title        = {Eligibility traces for off-policy policy evaluation},
	author       = {Precup, Doina},
	year         = 2000,
	journal      = {Computer Science Department Faculty Publication Series},
	pages        = 80
}
@inproceedings{rahaman2019spectral,
	title        = {On the spectral bias of neural networks},
	author       = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {5301--5310},
	organization = {PMLR}
}
@article{richards2018lyapunov,
	title        = {The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems},
	author       = {Richards, Spencer M and Berkenkamp, Felix and Krause, Andreas},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1808.00924}
}
@inproceedings{schodl2000video,
	title        = {Video textures},
	author       = {Sch{\"o}dl, Arno and Szeliski, Richard and Salesin, David H and Essa, Irfan},
	year         = 2000,
	booktitle    = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques},
	pages        = {489--498},
	organization = {ACM Press/Addison-Wesley Publishing Co.}
}
@inproceedings{shen2020uma,
	title        = {Model-based Policy Optimization with Unsupervised Model Adaptation},
	author       = {Shen, Jian and Zhao, Han and Zhang, Weinan and Yu, Yong},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {2823--2834},
	url          = {https://proceedings.neurips.cc/paper/2020/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}
@article{shi2022pessimistic,
	title        = {Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
	author       = {Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2202.13890}
}
@article{sitzmann2020implicit,
	title        = {Implicit neural representations with periodic activation functions},
	author       = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {7462--7473}
}
@book{spong2006robot,
	title        = {Robot modeling and control},
	author       = {Spong, Mark W and Hutchinson, Seth and Vidyasagar, Mathukumalli and others},
	year         = 2006
}
@inproceedings{sutton2009fast,
	title        = {Fast gradient-descent methods for temporal-difference learning with linear function approximation},
	author       = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
	year         = 2009,
	booktitle    = {Proceedings of the 26th Annual International Conference on Machine Learning},
	pages        = {993--1000}
}
@article{sutton2016emphatic,
	title        = {An emphatic approach to the problem of off-policy temporal-difference learning},
	author       = {Sutton, Richard S and Mahmood, A Rupam and White, Martha},
	year         = 2016,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR.org},
	volume       = 17,
	pages        = {2603--2631}
}
@book{sutton2020reinforcement,
	title        = {Reinforcement learning: An introduction},
	author       = {Sutton, Richard S and Barto, Andrew G},
	year         = 2020,
	publisher    = {MIT press},
	edition      = {Second Edition}
}
@article{taylor2019episodic,
	title        = {Episodic Learning with Control Lyapunov Functions for Uncertain Robotic Systems},
	author       = {Taylor, Andrew J and Dorobantu, Victor D and Le, Hoang M and Yue, Yisong and Ames, Aaron D},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1903.01577}
}
@inproceedings{tianhe2021combo,
	title        = {COMBO: Conservative Offline Model-Based Policy Optimization},
	author       = {Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {28954--28967},
	url          = {https://proceedings.neurips.cc/paper/2021/file/f29a179746902e331572c483c45e5086-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@inproceedings{tikhonov1943stability,
	title        = {On the stability of inverse problems},
	author       = {Tikhonov, Andrey Nikolayevich},
	year         = 1943,
	booktitle    = {Dokl. Akad. Nauk SSSR},
	volume       = 39,
	pages        = {195--198}
}
@article{tsitsiklis1996analysis,
	title        = {An analysis of temporal-difference learning with function approximation},
	author       = {Tsitsiklis, JN and Van Roy, B},
	year         = 1996,
	journal      = {Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep}
}
@inproceedings{umlauft2017learning,
	title        = {Learning stable stochastic nonlinear dynamical systems},
	author       = {Umlauft, Jonas and Hirche, Sandra},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages        = {3502--3510},
	organization = {JMLR. org}
}
@misc{vanderplas_2017,
	title        = {Triple Pendulum CHAOS!},
	author       = {VanderPlas, Jake},
	year         = 2017,
	month        = {Mar},
	journal      = {Pythonic Perambulations},
	howpublished = {\url{http://jakevdp.github.io/blog/2017/03/08/triple-pendulum-chaos/}}
}
@article{wang2020statistical,
	title        = {What are the statistical limits of offline RL with linear function approximation?},
	author       = {Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.11895}
}
@article{watkins1992q,
	title        = {Q-learning},
	author       = {Watkins, Christopher JCH and Dayan, Peter},
	year         = 1992,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 8,
	number       = 3,
	pages        = {279--292}
}
@article{yu2017convergence,
	title        = {On convergence of some gradient-based temporal-differences algorithms for off-policy learning},
	author       = {Yu, Huizhen},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1712.09652}
}
@article{yu2020mopo,
	title        = {Mopo: Model-based offline policy optimization},
	author       = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {14129--14142}
}
@inproceedings{zhang2020provably,
	title        = {Provably convergent two-timescale off-policy actor-critic with function approximation},
	author       = {Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
	year         = 2020,
	booktitle    = {International Conference on Machine Learning},
	pages        = {11204--11213},
	organization = {PMLR}
}
@article{zhang2021breaking,
	title        = {Breaking the Deadly Triad with a Target Network},
	author       = {Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2101.08862},
	url          = {https://arxiv.org/abs/2101.08862},
	eprinttype   = {arXiv},
	eprint       = {2101.08862}
}
