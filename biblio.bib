@inproceedings{amos2017input,
  title        = {Input convex neural networks},
  author       = {Amos, Brandon and Xu, Lei and Kolter, J Zico},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {146--155},
  year         = {2017},
  organization = {JMLR.org}
}

@article{amos2020svg,
  author        = {Brandon Amos and Samuel Stanton and Denis Yarats and Andrew Gordon Wilson},
  title         = {On the model-based stochastic value gradient for continuous reinforcement learning},
  journal       = {CoRR},
  volume        = {abs/2008.12775},
  year          = {2020},
  url           = {https://arxiv.org/abs/2008.12775},
  archiveprefix = {arXiv},
  eprint        = {2008.12775},
  timestamp     = {Wed, 16 Sep 2020 11:20:03 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2008-12775.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@techreport{baird1993counterexample,
  title       = {Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems},
  author      = {Williams, Ronald J and Baird III, Leemon C},
  year        = {1993},
  institution = {Citeseer}
}

@inproceedings{blocher2017learning,
  title        = {Learning stable dynamical systems using contraction theory},
  author       = {Blocher, Caroline and Saveriano, Matteo and Lee, Dongheui},
  booktitle    = {2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)},
  pages        = {124--129},
  year         = {2017},
  organization = {IEEE}
}

@inproceedings{boots2008constraint,
  title     = {A constraint generation approach to learning stable linear dynamical systems},
  author    = {Boots, Byron and Gordon, Geoffrey J and Siddiqi, Sajid M},
  booktitle = {Advances in neural information processing systems},
  pages     = {1329--1336},
  year      = {2008}
}


@article{buckman2018steve,
  author        = {Jacob Buckman and
                   Danijar Hafner and
                   George Tucker and
                   Eugene Brevdo and
                   Honglak Lee},
  title         = {Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value
                   Expansion},
  journal       = {CoRR},
  volume        = {abs/1807.01675},
  year          = {2018},
  url           = {http://arxiv.org/abs/1807.01675},
  archiveprefix = {arXiv},
  eprint        = {1807.01675},
  timestamp     = {Mon, 13 Aug 2018 16:49:01 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1807-01675.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}


@article{burer2003nonlinear,
  title     = {A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
  author    = {Burer, Samuel and Monteiro, Renato DC},
  journal   = {Mathematical Programming},
  volume    = {95},
  number    = {2},
  pages     = {329--357},
  year      = {2003},
  publisher = {Springer}
}

@article{chen2018optimal,
  title   = {Optimal Control Via Neural Networks: A Convex Approach},
  author  = {Chen, Yize and Shi, Yuanyuan and Zhang, Baosen},
  journal = {arXiv preprint arXiv:1805.11835},
  year    = {2018}
}
@inproceedings{chow2018lyapunov,
  title     = {A lyapunov-based approach to safe reinforcement learning},
  author    = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8092--8101},
  year      = {2018}
}

@inproceedings{chua2018pets,
  author    = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
  url       = {https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf},
  volume    = {31},
  year      = {2018}
}

@article{diddigi2019convergent,
  title   = {A convergent off-policy temporal difference algorithm},
  author  = {Diddigi, Raghuram Bharadwaj and Kamanchi, Chandramouli and Bhatnagar, Shalabh},
  journal = {arXiv preprint arXiv:1911.05697},
  year    = {2019}
}
@inproceedings{du2017stochastic,
  title        = {Stochastic variance reduction methods for policy evaluation},
  author       = {Du, Simon S and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1049--1058},
  year         = {2017},
  organization = {PMLR}
}


@article{Feinberg2018ModelBasedVE,
  title   = {Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning},
  author  = {Vladimir Feinberg and Alvin Wan and I. Stoica and Michael I. Jordan and Joseph E. Gonzalez and Sergey Levine},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1803.00101}
}

@inproceedings{fujimoto2019off,
  title        = {Off-policy deep reinforcement learning without exploration},
  author       = {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle    = {International conference on machine learning},
  pages        = {2052--2062},
  year         = {2019},
  organization = {PMLR}
}

@inproceedings{gal2016improving,
  title     = {Improving PILCO with Bayesian neural network dynamics models},
  author    = {Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle = {Data-Efficient Machine Learning workshop, ICML},
  volume    = {4},
  year      = {2016}
}

@inproceedings{gelada2019off,
  title     = {Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author    = {Gelada, Carles and Bellemare, Marc G},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {33},
  pages     = {3647--3655},
  year      = {2019}
}

@inproceedings{gu2016continuous,
  title     = {Continuous deep q-learning with model-based acceleration},
  author    = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  booktitle = {International Conference on Machine Learning},
  pages     = {2829--2838},
  year      = {2016}
}

@article{hasselt2021expected,
  title   = {Expected eligibility traces},
  author  = {van Hasselt, Hado and Madjiheurem, Sephora and Hessel, Matteo and Silver, David and Barreto, Andr{\'e} and Borsa, Diana},
  journal = {arXiv preprint arXiv:2007.01839},
  year    = {2021}
}

@inproceedings{he2015delving,
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {1026--1034},
  year      = {2015}
}

@inproceedings{janner2019mbpo,
  author    = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  title     = {When to Trust Your Model: Model-Based Policy Optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019}
}

@inproceedings{jiang2021emphatic,
  title     = {Emphatic Algorithms for Deep Reinforcement Learning},
  author    = {Jiang, Ray and Zahavy, Tom and Xu, Zhongwen and White, Adam and Hessel, Matteo and Blundell, Charles and Van Hasselt, Hado},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {5023--5033},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {7},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/jiang21j/jiang21j.pdf},
  url       = {https://proceedings.mlr.press/v139/jiang21j.html},
  abstract  = {Off-policy learning allows us to learn about possible policies of behavior from experience generated by a different behavior policy. Temporal difference (TD) learning algorithms can become unstable when combined with function approximation and off-policy sampling—this is known as the “deadly triad”. Emphatic temporal difference (ETD($\lambda$)) algorithm ensures convergence in the linear case by appropriately weighting the TD($\lambda$) updates. In this paper, we extend the use of emphatic methods to deep reinforcement learning agents. We show that naively adapting ETD($\lambda$) to popular deep reinforcement learning algorithms, which use forward view multi-step returns, results in poor performance. We then derive new emphatic algorithms for use in the context of such algorithms, and we demonstrate that they provide noticeable benefits in small problems designed to highlight the instability of TD methods. Finally, we observed improved performance when applying these algorithms at scale on classic Atari games from the Arcade Learning Environment.}
}

@article{jiang2021learning,
  title   = {Learning Expected Emphatic Traces for Deep RL},
  author  = {Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and van Hasselt, Hado},
  journal = {arXiv preprint arXiv:2107.05405},
  year    = {2021}
}

@article{jiang2022learning,
  title        = {Learning Expected Emphatic Traces for Deep RL},
  volume       = {36},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20660},
  doi          = {10.1609/aaai.v36i6.20660},
  abstractnote = {Off-policy sampling and experience replay are key for improving sample efficiency and scaling model-free temporal difference learning methods. When combined with function approximation, such as neural networks, this combination is known as the deadly triad and is potentially unstable. Recently, it has been shown that stability and good performance at scale can be achieved by combining emphatic weightings and multi-step updates. This approach, however, is generally limited to sampling complete trajectories in order, to compute the required emphatic weighting. In this paper we investigate how to combine emphatic weightings with non-sequential, off-line data sampled from a replay buffer. We develop a multi-step emphatic weighting that can be combined with replay, and a time-reversed n-step TD learning algorithm to learn the required emphatic weighting. We show that these state weightings reduce variance compared with prior approaches, while providing convergence guarantees. We tested the approach at scale on Atari 2600 video games, and observed that the new X-ETD(n) agent improved over baseline agents, highlighting both the scalability and broad applicability of our approach.},
  number       = {6},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and Hasselt, Hado van},
  year         = {2022},
  month        = {6},
  pages        = {7015-7023}
}

@book{khalil2002nonlinear,
  title     = {Nonlinear systems},
  author    = {Khalil, Hassan K and Grizzle, Jessy W},
  volume    = {3},
  year      = {2002},
  publisher = {Prentice hall Upper Saddle River, NJ}
}

@article{khansari2011learning,
  title     = {Learning stable nonlinear dynamical systems with gaussian mixture models},
  author    = {Khansari-Zadeh, S Mohammad and Billard, Aude},
  journal   = {IEEE Transactions on Robotics},
  volume    = {27},
  number    = {5},
  pages     = {943--957},
  year      = {2011},
  publisher = {IEEE}
}

@article{kidambi2020morel,
  title   = {Morel: Model-based offline reinforcement learning},
  author  = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {21810--21823},
  year    = {2020}
}

@article{kingma2013auto,
  title   = {Auto-encoding variational bayes},
  author  = {Kingma, Diederik P and Welling, Max},
  journal = {arXiv preprint arXiv:1312.6114},
  year    = {2013}
}

@inproceedings{Kingma2014,
  added-at    = {2020-10-15T14:36:56.000+0200},
  author      = {Kingma, Diederik P. and Welling, Max},
  biburl      = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle   = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint      = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash   = {a626a9d77a123c52405a08da983203cb},
  intrahash   = {42e5be6faa01cba2587f4907ac99dce8},
  keywords    = {cs.LG stat.ML vae},
  timestamp   = {2021-02-01T17:13:18.000+0100},
  title       = {{Auto-Encoding Variational Bayes}},
  year        = 2014
}

@article{kolter2011fixed,
  title   = {The fixed points of off-policy TD},
  author  = {Kolter, J},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {24},
  pages   = {2169--2177},
  year    = {2011}
}

@misc{kolternote2022,
  author       = {Kolter, Zico},
  howpublished = {private communication},
  year         = {2022},
  month        = {3}
}

@article{kumar19bear,
  author     = {Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
  title      = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  conference = {NeurIPS 2019},
  url        = {http://arxiv.org/abs/1906.00949}
}

@article{kumar2019stabilizing,
  title   = {Stabilizing off-policy q-learning via bootstrapping error reduction},
  author  = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {32},
  year    = {2019}
}

@article{kumar2020discor,
  title   = {Discor: Corrective feedback in reinforcement learning via distribution correction},
  author  = {Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal = {arXiv preprint arXiv:2003.07305},
  year    = {2020}
}

@book{la2012stability,
  title     = {Stability by Liapunov's Direct Method with Applications by Joseph L Salle and Solomon Lefschetz},
  author    = {La Salle, Joseph and Lefschetz, Solomon},
  volume    = {4},
  year      = {2012},
  publisher = {Elsevier}
}

@inproceedings{laroche2019safe,
  title        = {Safe policy improvement with baseline bootstrapping},
  author       = {Laroche, Romain and Trichelair, Paul and Des Combes, Remi Tachet},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3652--3661},
  year         = {2019},
  organization = {PMLR}
}

@inproceedings{lee2021sunrise,
  title        = {Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
  author       = {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle    = {International Conference on Machine Learning},
  pages        = {6131--6141},
  year         = {2021},
  organization = {PMLR}
}

@article{levine2020survey,
  author     = {Sergey Levine and
                Aviral Kumar and
                George Tucker and
                Justin Fu},
  title      = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives
                on Open Problems},
  journal    = {CoRR},
  volume     = {abs/2005.01643},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.01643},
  eprinttype = {arXiv},
  eprint     = {2005.01643},
  timestamp  = {Fri, 08 May 2020 15:04:04 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-01643.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{mahadevan2014proximal,
  title   = {Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces},
  author  = {Mahadevan, Sridhar and Liu, Bo and Thomas, Philip and Dabney, Will and Giguere, Steve and Jacek, Nicholas and Gemp, Ian and Liu, Ji},
  journal = {arXiv preprint arXiv:1405.6757},
  year    = {2014}
}

@article{manek2019stable,
  title   = {Learning stable deep dynamics models},
  author  = {Manek, Gaurav and Kolter, J Zico},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

@article{manek2020stablevalue,
  title   = {Notes on Provably Stable Value Functions},
  author  = {Manek, Gaurav and Kolter, Zico},
  journal = {unpublished},
  year    = {2020},
  month   = {Oct.}
}

@article{manek2021mve,
  title   = {Notes on Model-Value Expansion},
  author  = {Manek, Gaurav and Kolter, Zico},
  journal = {unpublished},
  year    = {2021},
  month   = {Oct.}
}

@inproceedings{manek2022pitfalls,
  title     = {The Pitfalls of Regularization in Off-Policy {TD} Learning},
  author    = {Gaurav Manek and J Zico Kolter},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year      = {2022},
  url       = {https://openreview.net/forum?id=vK53GLZJes8}
}

@inproceedings{mishra2017prediction,
  title        = {Prediction and control with temporal segment models},
  author       = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {2459--2468},
  year         = {2017},
  organization = {JMLR. org}
}

@article{mnih2015humanlevel,
  added-at    = {2015-08-26T14:46:40.000+0200},
  author      = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl      = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash   = {eac59980357d99db87b341b61ef6645f},
  intrahash   = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn        = {00280836},
  journal     = {Nature},
  keywords    = {deep learning toread},
  month       = feb,
  number      = 7540,
  pages       = {529--533},
  publisher   = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp   = {2015-08-26T14:46:40.000+0200},
  title       = {Human-level control through deep reinforcement learning},
  url         = {http://dx.doi.org/10.1038/nature14236},
  volume      = 518,
  year        = 2015
}

@inproceedings{nagabandi2018neural,
  title        = {Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author       = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle    = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {7559--7566},
  year         = {2018},
  organization = {IEEE}
}

@inproceedings{papachristodoulou2002construction,
  title        = {On the construction of Lyapunov functions using the sum of squares decomposition},
  author       = {Papachristodoulou, Antonis and Prajna, Stephen},
  booktitle    = {Proceedings of the 41st IEEE Conference on Decision and Control, 2002.},
  volume       = {3},
  pages        = {3482--3487},
  year         = {2002},
  organization = {IEEE}
}

@phdthesis{parrilo2000structured,
  title  = {Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization},
  author = {Parrilo, Pablo A},
  year   = {2000},
  school = {California Institute of Technology}
}

@article{peng2019advantage,
  title   = {Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author  = {Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal = {arXiv preprint arXiv:1910.00177},
  year    = {2019}
}

@inproceedings{rahaman2019spectral,
  title        = {On the spectral bias of neural networks},
  author       = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle    = {International Conference on Machine Learning},
  pages        = {5301--5310},
  year         = {2019},
  organization = {PMLR}
}

@article{richards2018lyapunov,
  author  = {Richards, Spencer M and Berkenkamp, Felix and Krause, Andreas},
  title   = {The lyapunov neural network: Adaptive stability certification for safe learning of dynamic systems},
  journal = {arXiv preprint arXiv:1808.00924},
  year    = {2018}
}

@inproceedings{schodl2000video,
  title        = {Video textures},
  author       = {Sch{\"o}dl, Arno and Szeliski, Richard and Salesin, David H and Essa, Irfan},
  booktitle    = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques},
  pages        = {489--498},
  year         = {2000},
  organization = {ACM Press/Addison-Wesley Publishing Co.}
}

@inproceedings{shen2020uma,
  author    = {Shen, Jian and Zhao, Han and Zhang, Weinan and Yu, Yong},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {2823--2834},
  publisher = {Curran Associates, Inc.},
  title     = {Model-based Policy Optimization with Unsupervised Model Adaptation},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{sitzmann2020implicit,
  title   = {Implicit neural representations with periodic activation functions},
  author  = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {7462--7473},
  year    = {2020}
}

@book{spong2006robot,
  title  = {Robot modeling and control},
  author = {Spong, Mark W and Hutchinson, Seth and Vidyasagar, Mathukumalli and others},
  year   = {2006}
}

@inproceedings{sutton2009fast,
  title     = {Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author    = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  pages     = {993--1000},
  year      = {2009}
}

@article{sutton2016emphatic,
  title     = {An emphatic approach to the problem of off-policy temporal-difference learning},
  author    = {Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal   = {The Journal of Machine Learning Research},
  volume    = {17},
  pages     = {2603--2631},
  year      = {2016},
  publisher = {JMLR.org}
} 

@book{sutton2020reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2020},
  edition   = {Second Edition},
  publisher = {MIT press}
}

@article{taylor2019episodic,
  title   = {Episodic Learning with Control Lyapunov Functions for Uncertain Robotic Systems},
  author  = {Taylor, Andrew J and Dorobantu, Victor D and Le, Hoang M and Yue, Yisong and Ames, Aaron D},
  journal = {arXiv preprint arXiv:1903.01577},
  year    = {2019}
}

@inproceedings{tianhe2021combo,
  author    = {Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {28954--28967},
  publisher = {Curran Associates, Inc.},
  title     = {COMBO: Conservative Offline Model-Based Policy Optimization},
  url       = {https://proceedings.neurips.cc/paper/2021/file/f29a179746902e331572c483c45e5086-Paper.pdf},
  volume    = {34},
  year      = {2021}
}

@inproceedings{tikhonov1943stability,
  title     = {On the stability of inverse problems},
  author    = {Tikhonov, Andrey Nikolayevich},
  booktitle = {Dokl. Akad. Nauk SSSR},
  volume    = {39},
  pages     = {195--198},
  year      = {1943}
}

@article{tsitsiklis1996analysis,
  title   = {An analysis of temporal-difference learning with function approximation},
  author  = {Tsitsiklis, JN and Van Roy, B},
  journal = {Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep},
  year    = {1996}
}

@inproceedings{umlauft2017learning,
  title        = {Learning stable stochastic nonlinear dynamical systems},
  author       = {Umlauft, Jonas and Hirche, Sandra},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {3502--3510},
  year         = {2017},
  organization = {JMLR. org}
}

@misc{vanderplas_2017,
  title        = {Triple Pendulum CHAOS!},
  howpublished = {\url{http://jakevdp.github.io/blog/2017/03/08/triple-pendulum-chaos/}},
  journal      = {Pythonic Perambulations},
  author       = {VanderPlas, Jake},
  year         = {2017},
  month        = {Mar}
}

@article{wang2020statistical,
  title   = {What are the statistical limits of offline RL with linear function approximation?},
  author  = {Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
  journal = {arXiv preprint arXiv:2010.11895},
  year    = {2020}
}

@article{yu2017convergence,
  title   = {On convergence of some gradient-based temporal-differences algorithms for off-policy learning},
  author  = {Yu, Huizhen},
  journal = {arXiv preprint arXiv:1712.09652},
  year    = {2017}
}


@article{yu2020mopo,
  title   = {Mopo: Model-based offline policy optimization},
  author  = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {14129--14142},
  year    = {2020}
}

@inproceedings{zhang2020provably,
  title        = {Provably convergent two-timescale off-policy actor-critic with function approximation},
  author       = {Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
  booktitle    = {International Conference on Machine Learning},
  pages        = {11204--11213},
  year         = {2020},
  organization = {PMLR}
}


@article{zhang2021breaking,
  author     = {Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
  title      = {Breaking the Deadly Triad with a Target Network},
  journal    = {CoRR},
  volume     = {abs/2101.08862},
  year       = {2021},
  url        = {https://arxiv.org/abs/2101.08862},
  eprinttype = {arXiv},
  eprint     = {2101.08862}
}
