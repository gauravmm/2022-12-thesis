@article{kolter2011fixed,
  title   = {The fixed points of off-policy TD},
  author  = {Kolter, J},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {24},
  pages   = {2169--2177},
  year    = {2011}
}

@article{zhang2021breaking,
  author     = {Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon},
  title      = {Breaking the Deadly Triad with a Target Network},
  journal    = {CoRR},
  volume     = {abs/2101.08862},
  year       = {2021},
  url        = {https://arxiv.org/abs/2101.08862},
  eprinttype = {arXiv},
  eprint     = {2101.08862}
}

@inproceedings{zhang2020provably,
  title        = {Provably convergent two-timescale off-policy actor-critic with function approximation},
  author       = {Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
  booktitle    = {International Conference on Machine Learning},
  pages        = {11204--11213},
  year         = {2020},
  organization = {PMLR}
}

@techreport{baird1993counterexample,
  title       = {Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems},
  author      = {Williams, Ronald J and Baird III, Leemon C},
  year        = {1993},
  institution = {Citeseer}
}

@book{sutton2020reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2020},
  edition   = {Second Edition},
  publisher = {MIT press}
}

@article{mahadevan2014proximal,
  title={Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces},
  author={Mahadevan, Sridhar and Liu, Bo and Thomas, Philip and Dabney, Will and Giguere, Steve and Jacek, Nicholas and Gemp, Ian and Liu, Ji},
  journal={arXiv preprint arXiv:1405.6757},
  year={2014}
}

@article{yu2017convergence,
  title={On convergence of some gradient-based temporal-differences algorithms for off-policy learning},
  author={Yu, Huizhen},
  journal={arXiv preprint arXiv:1712.09652},
  year={2017}
}

@inproceedings{du2017stochastic,
  title={Stochastic variance reduction methods for policy evaluation},
  author={Du, Simon S and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong},
  booktitle={International Conference on Machine Learning},
  pages={1049--1058},
  year={2017},
  organization={PMLR}
}

@article{diddigi2019convergent,
  title={A convergent off-policy temporal difference algorithm},
  author={Diddigi, Raghuram Bharadwaj and Kamanchi, Chandramouli and Bhatnagar, Shalabh},
  journal={arXiv preprint arXiv:1911.05697},
  year={2019}
}

@inproceedings{gelada2019off,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3647--3655},
  year={2019}
}

@article{kumar2020discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2003.07305},
  year={2020}
}

@article{sutton2016emphatic,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  pages={2603--2631},
  year={2016},
  publisher={JMLR.org}
}

@inproceedings{sutton2009fast,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={993--1000},
  year={2009}
}

@article{tsitsiklis1996analysis,
  title={An analysis of temporal-difference learning with function approximation},
  author={Tsitsiklis, JN and Van Roy, B},
  journal={Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep},
  year={1996}
}

@inproceedings{tikhonov1943stability,
  title={On the stability of inverse problems},
  author={Tikhonov, Andrey Nikolayevich},
  booktitle={Dokl. Akad. Nauk SSSR},
  volume={39},
  pages={195--198},
  year={1943}
}

@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}

@article{hasselt2021expected,
  title={Expected eligibility traces},
  author={van Hasselt, Hado and Madjiheurem, Sephora and Hessel, Matteo and Silver, David and Barreto, Andr{\'e} and Borsa, Diana},
  journal={arXiv preprint arXiv:2007.01839},
  year={2021}
}

@article{jiang2021learning,
  title={Learning Expected Emphatic Traces for Deep RL},
  author={Jiang, Ray and Zhang, Shangtong and Chelu, Veronica and White, Adam and van Hasselt, Hado},
  journal={arXiv preprint arXiv:2107.05405},
  year={2021}
}

@inproceedings{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  booktitle={International Conference on Machine Learning},
  pages={30--37},
  year={1995},
  publisher={Elsevier}
}

@inproceedings{kumar2022dr,
title={{DR}3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization},
author={Aviral Kumar and Rishabh Agarwal and Tengyu Ma and Aaron Courville and George Tucker and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=POvMvLi91f}
}


@InProceedings{chaudhuri2022first,
  title = 	 {First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach},
  author =       {Wagenmaker, Andrew J and Chen, Yifang and Simchowitz, Max and Du, Simon and Jamieson, Kevin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {22384--22429},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wagenmaker22a/wagenmaker22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wagenmaker22a.html},
  abstract = 	 {Obtaining first-order regret bounds—regret bounds scaling not as the worst-case but with some measure of the performance of the optimal policy on a given instance—is a core question in sequential decision-making. While such bounds exist in many settings, they have proven elusive in reinforcement learning with large state spaces. In this work we address this gap, and show that it is possible to obtain regret scaling as $\widetilde{\mathcal{O}}(\sqrt{d^3 H^3 \cdot V_1^\star \cdot K} + d^{3.5}H^3\log K )$ in reinforcement learning with large state spaces, namely the linear MDP setting. Here $V_1^\star$ is the value of the optimal policy and $K$ is the number of episodes. We demonstrate that existing techniques based on least squares estimation are insufficient to obtain this result, and instead develop a novel robust self-normalized concentration bound based on the robust Catoni mean estimator, which may be of independent interest.}
}

@article{levine2020offline,
  author    = {Sergey Levine and
               Aviral Kumar and
               George Tucker and
               Justin Fu},
  title     = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives
               on Open Problems},
  journal   = {CoRR},
  volume    = {abs/2005.01643},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.01643},
  eprinttype = {arXiv},
  eprint    = {2005.01643},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-01643.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle={International Conference on Machine Learning},
  pages={3061--3071},
  year={2020},
  organization={PMLR}
}
