
\section{Appendix}

\subsection{Example Details}

\subsubsection{``Vacuous'' models}
\label{sec:choiceoffailure}
Without $\ell_2$ regularization, our linear model fails with asymptotic error. As this penalizes the $\ell_2$-norm of the learned weights, this removes the asymptote and so we can no longer use the existence of an asymptote as evidence of failure. Instead, we propose a different definition of failure by noting that, in the limiting case, regularization drives the learned weights to zero ($\lim_{\eta\to\infty} w^*(\eta) = \vec 0$). The learned value function $\Phi\cdot\vec 0 = \vec 0$ has no information about the true value function. We argue that if the error with any $\eta\in\mathbb R^+$ is never better than this case then the model is vacuous and hence adopt the threshold error of $\|\Phi\cdot\vec 0 - V\| = \|V\|$ to call a model vacuous. This explains the failure condition in Equation~\ref{eqn:vacuoustd}.


\subsubsection{Small-Eta Error}
Our simplified example allows us to show this easily.

\begin{example}\label{ex:badeta2}
  When TD is regularized, the model may diverge around (typically small) values of $\eta$.
  \proof

  We set $p=0.9$ and solve for $\det(A+\eta I)=0$:
  \begin{align}
    0 & = 100\eta^2+47.4p\eta +1.85\eta - 1.30p^2 + 0.927p
    \\  \eta & = 0.00482577 \quad \lor \quad \eta = -0.45
  \end{align}
  Note that the denominator of $g(p,\eta)$ is proportional to $\det(A+\eta I)$, and so $g(0.9,\eta)$--and the error at the TD fixed point--can be made arbitrarily large by selecting $\eta$ close to $4.83\times 10^{-3}$. As this is the only positive root, the model does not diverge at other values.
\end{example}


\subsubsection{Emphatic approaches and our counterexample}
\label{app:emphatic}
\begin{figure}
  \input{Pitfalls/emphasistd/emphasistd.tex}
  \caption{Regularization distorts the emphasis model (left), which induces the value function (right) to move to a singularity. Unregularized models are shown in red, regularized models in blue. Regularization can interact with emphasis models to significantly worsen learned value functions. }
  \label{fig:emphasisplots}
\end{figure}

We use an MP with the same transition function as in Figure~\ref{fig:mdp_illustration}, with separate bases $\Phi_m$ and $\Phi_v$ for the emphasis and value stages respectively. We assume that our interest in all states is uniformly $i=1$.

We begin by setting the off-policy sampling distribution of $\mu=[.2~.2~.6]$, used as the diagonal matrix $D_\mu=\text{diag}(\mu)$. Thanks to the simple structure of our example, we know the emphasis is $m = \frac{i}{1-\gamma} \cdot \pi D_\mu^{-1} \propto \left(\sfrac{5}{4}, \sfrac{5}{4}, \sfrac{5}{6}\right)$. We select a basis that allows us to represent this emphasis:
\begin{align}
  \Phi_m                                           & = \begin{bmatrix}\sfrac{5}{4} & 0 \\ 0 & -\sfrac{1}{100}\cdot\sfrac{5}{4} \\ \sfrac{5}{12} & -\sfrac{1}{100}\cdot\sfrac{5}{12} \end{bmatrix}
  \intertext{We deliberately choose $\Phi_m$ to have a poor condition number for reasons that will become apparent later. We can represent $c \cdot \left(\sfrac{5}{4}, \sfrac{5}{4}, \sfrac{5}{6}\right)$ exactly for any constant $c$:}
  \Phi_m \cdot (1, -100) \cdot c                   & = c\cdot \left(\sfrac{5}{4}, \sfrac{5}{4}, \sfrac{5}{6}\right)
  \intertext{Using Equation 5 from \cite{zhang2020provably}, we define the matrices:}
  C_m = \Phi_m^\top D_\mu \Phi_m                   & = \begin{bmatrix}
                                                         0.417 & -1.04\times 10^{-3} \\ -1.04\times 10^{-3} & 4.17\times 10^{-5}
                                                       \end{bmatrix}                                                                     \\
  A_m = \Phi_m^\top (I-\gamma P^\top) D_\mu \Phi_m & = \begin{bmatrix}
                                                         0.159 & 1.536\times 10^{-3} \\ 1.536\times 10^{-3} & 1.59\times 10^{-5}
                                                       \end{bmatrix}
  \intertext{And we apply these to the formulation in Lemma 3 and compute the emphasis weights as a function of the regularization $w_m : \mathbb R^+_0 \to \mathbb R^+$:}
  w_m^*(\eta)                                      & = (A_m^\top C_m^{-1} A_m + \eta I)^{-1} A_m^\top C_m^{-1} \Phi_m^\top D i
\end{align}

We can then use this to compute the new apparent distribution $\upsilon$, which is the effective distribution that the updates to the value model see, and it is equal to the emphasis multiplied by the off-policy distribution.
\begin{align}
  \upsilon(\eta)           & = \Phi_m \cdot w_m^*(\eta) \cdot D
  \intertext{Without any regularization, this should be exactly equal to the on-policy distribution. }
  \upsilon(0)              & = [0.25~0.25~0.5] \equiv \pi
  \intertext{When we compute this value with a small amount of regularization $\eta=2\times10^{-4}$, we observe that the apparent distribution drifts far away from the on-policy distribution.}
  \upsilon(2\times10^{-4}) & = [0.44~0.06~0.5]
\end{align}
The proximate cause of this is the poor condition number of $C$, caused by the $\frac{1}{100}$ scale factor applied to the second column of $\Phi_m$. This allows $\eta$ to affect different columns by different (relative) amounts in the definition of $w^*(\eta)$, which pushes it away from the symmetric solution. See this error shift in Figure~\ref{fig:emphimp}.

So far, we have shown how regularization causes a shift in the apparent distribution that the TD updates see. To complete the example we show how this moves the fixed point of the value function away from a stable point into an asymptote where it may grow without bounds. This second phase follows in the same pattern as the first phase, starting with the desired value function: $V=[1~2.69~1.05]$ and a basis that can almost exactly represent the value function:
\begin{align*}
  \Phi_v                                                      & = \begin{bmatrix}
                                                                    1 & 0 \\ 0 & -2.69 \\ \sfrac12(\epsilon+1.05) & -\sfrac12(\epsilon+1.05)
                                                                  \end{bmatrix} \\
  \epsilon                                                    & = 2\times 10^{-4}
  \intertext{We use this basis to compute the state-rewards $R=(I-\gamma P)V=[-0.43~1.26~-0.38]$ and define the matrices $A_v$ and $C_v$ and the solution $w^*_v(\eta)$:}
  A_v                                                         & = \Phi_v^\top (I-\gamma P^\top) D \Phi_v                                  \\
  C_v                                                         & = \Phi_v^\top D \Phi_v                                                    \\
  w_v^*(\eta)                                                 & = (A_v^\top C_v^{-1} A_v + \eta I)^{-1} A_v^\top C_v^{-1} \Phi_v^\top D R
  \intertext{We can use this solution to compute the error between the value function and the true values, $\|\Phi_v w_v(\eta) - V\|$. First, under the corrected distribution without regularization $\upsilon(0) \equiv \pi$:}
  \Phi_v w_v^*(0)|_{D=\text{diag}(\upsilon(0))}               & = 0.000865
  \intertext{Then, with regularization in the value function (but not in the emphasis function):}
  \Phi_v w_v^*(2\times 10^{-4})|_{D=\text{diag}(\upsilon(0))} & = 0.0162
  \intertext{Then, under the apparent distribution $\upsilon$ induced by use of regularization in the emphasis function, without and with regularization:}
  \Phi_v w_v^*(0)|_{D=\text{diag}(\upsilon(2\times 10^{-4}))} & = 418.601
  \\ \Phi_v w_v^*(2\times 10^{-4})|_{D=\text{diag}(\upsilon(2\times 10^{-4}))} & = 3.00
\end{align*}
It is immediately obvious that the use of regularization in the emphasis function causes the learned value function to be incorrect. Including a regularizing term in the value estimate is not sufficient to fix the value function. This completes the example. \qed

\subsubsection{Kolter's non-expansion condition and our counterexample. }
In the construction of COF-PAC, a key assumption made is that both the emphasis and value models are not subject to runaway TD \cite[asm.~4]{zhang2020provably}. Specifically, they make the strong assumption that Kolter's relaxed-contraction condition \cite[eqn.~10]{kolter2011fixed} holds in the emphasis model at $\mu$ and value model at $\upsilon$. Kolter's condition selects a convex subset of distributions under which one-step transition followed by projection onto $\Phi$ is non-expansive. We illustrate these regions in Figure~\ref{fig:koldercond}. Even in the one-dimensional parameterization shown, this condition only holds in a small sub-region of the space and therefore appears to be a very strong condition. Empirically determining if such a condition holds (or training models to enforce it) may be possible with TD-DO \cite[sec~4.1]{kolter2011fixed}, but it is not clear how that method interacts with regularization. \label{sec:nosingularity}

\begin{figure}
  \centering
  \input{Pitfalls/koltercond/koltercond.tex}
  \caption{Kolter's non-expansion condition holds in the shaded region of each graph. }
  \label{fig:koldercond}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applied to multi-layer networks}
\label{sec:multilayerapp}

We also use a variant of our example to study how the deadly triad appears in multi-layer networks. As illustrated in Figure~\ref{fig:multilayermdp}, we replace each self-loop with two additional states, forming a clique with the original state. The resultant MP has $n=9$ states; we define a deterministic observation function $o : \mathcal S \to \mathbb B^6$. where each state is encoded as the concatenation of the one-hot vector of its subscripts. The value function is assigned pseudo-randomly in range $[-1, 1]$, and a consistent reward function is assigned.
We select the family of sampling distributions $\mu \propto [4p,~1p,~1p,4p,~1p,~1p,8(1-p),~4(1-p),~4(1-p)]$, where the on-policy distribution is at $p=0.5$.
%The on-policy distribution is $\pi = \sfrac{1}{28} [4,~1,~1,4,~1,~1,8,~4,~4]$; 

We wish to learn the model with a two-layer network with $k < n$ nodes in the inner layer.
We define the network as $f(o(s_{i,j})) = \tan^{-1}(o(s_{i, j}) * \omega_1) * \omega_2$. The parameters $\omega_1 \in \mathbb R^{6\times k}$, $\omega_2 \in \mathbb R^{k \times 1}$ are trained to convergence using simple TD updates with semi-gradient updates, a fixed learning rate, and without a target network.

In addition to the example in Figure~\ref{fig:mlperfeta}, we present an additional example in Figure~\ref{fig:twomultilayerperfs}. The same Markov process, at a different off-policy distribution, attains a curve where the non-vacuous region lies before the divergent region, similar to the second row in Figure \ref{fig:etagraph}. An added observation is that these two graphs are mutually incompatible -- there is no fixed $\eta$ that can simultaneously do better than vacuity in both, which promotes the idea of testing multiple regularization parameters or using an adaptive regularization scheme.

\begin{figure}[b]
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{Pitfalls/multilayerperf/multilayerperf2a.tex}
    \caption{$p=0.31$ (Same as Figure~
      \ref{fig:mlperfeta})}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{Pitfalls/multilayerperf/multilayerperf2b.tex}
    \caption{$p=0.95$}
  \end{subfigure}
  \caption{The relationship between error and $\eta$ at different off-policy distributions, showing mutually incompatible regularization behavior. The shaded range indicates the region between the 5th and 95th percentile of 100 differently-initialized models. }
  \label{fig:twomultilayerperfs}
\end{figure}

\subsection{Over-parameterization does not solve this problem}

Baird's counterexample \cite{baird1993counterexample} shows how, in the linear case, that off-policy divergence can also happen with over-parameterization, as long as some amount of function approximation occurs. It is not obvious that this conclusion persists in the neural network case, so we include an additional example showing that the parameterization doesn't resolve small-$\eta$ divergence.

In Figure~\ref{fig:twomultilayerperfs_k} we plot models with 3 to 13 nodes in the hidden layer. For reference, the MDP has 9 states, so some models under-parameterize and some models over-parameterize. We observe that, in the low-regularization regime, increasing the number of parameters improves the error slightly. However, increasing the number of parameters in the hidden layer does not change the behavior in the the small-$\eta$ divergence region.

\begin{figure}[b]
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{Pitfalls/multilayerperf_k/multilayerperf1a.tex}
    \caption{$p=0.31$}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \input{Pitfalls/multilayerperf_k/multilayerperf1b.tex}
    \caption{$p=0.95$}
  \end{subfigure}
  \caption{The relationship between $\eta$ and error with different amount of model parameterization (with 3, 5, 7, 9, 11, 13, and 64 nodes in the hidden layer, corresponding to darkening colors.) }
  \label{fig:twomultilayerperfs_k}
\end{figure}

\subsubsection{Relationship to modern RL algorithms}

It is still not obvious how strongly this instability affects modern RL algorithms, which are also sensitive to a variety of other failure modes. Unlike our analysis, the sampling distribution changes during training, and regularization mechanisms are more complex than simple $\ell_2$ penalities. The exact relationship between the instabilities we study and RL algorithms is an open problem, but we offer two pieces of indirect evidence suggesting there is a link.

First, in the offline/batch RL literature, it is well-known that online RL algorithms naively applied can catastrophically fail if the learned policy is not consistent with the data distribution. This is known as the distribution shift problem, \cite[p.~26]{levine2020offline} and offline RL algorithms are generally constructed to explicitly address this. Second, when using experience replay buffers in online RL algorithms, policy quality generally improves when older transitions are more quickly evicted \cite{fedus2020revisiting}. However, there are multiple factors at work here, and it is not possible to separate out the instability from off-policy sampling from the remaining factors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Markov Processes}\label{sec:all_mdps}

We use a three-, five- and nine-state Markov Processes to generate examples for this paper. Here we give details of the construction of each example. Mathematica code for all examples is included in the supplementary material.

\subsection{Three-state}
The construction of the three-state MDP is described in Section~\ref{sec:introduce_example} and illustrated in Figure~\ref{fig:mdp_illustration}. This example is used in Examples~\ref{ex:withrr} and \ref{ex:emph} For completeness, the transition matrix is:
>\subsubsection{Nine-state}
This example is used to train neural networks. The construction is based on the three-state example and the construction is illustrated in Figures \ref{fig:mdp9_illustration} and \ref{fig:multilayermdp}. The transition matrix (with omitted zeros) is:
\begin{align}
  \begin{bmatrix}
    1 & 1 & 1 & 3 &   &   & 6 &   &   \\
    4 & 4 & 4 &   &   &   &   &   &   \\
    4 & 4 & 4 &   &   &   &   &   &   \\
    3 &   &   & 1 & 1 & 1 & 6 &   &   \\
      &   &   & 4 & 4 & 4 &   &   &   \\
      &   &   & 4 & 4 & 4 &   &   &   \\
    3 &   &   & 3 &   &   & 2 & 2 & 2 \\
      &   &   &   &   &   & 4 & 4 & 4 \\
      &   &   &   &   &   & 4 & 4 & 4 \\
  \end{bmatrix}
  \intertext{and the observation function that forces the neural network to approximate is:}
  o : \mathcal S\to \mathbb R^6 =
  \begin{bmatrix}
    1 & 0 & 0 & 1 & 0 & 0 \\
    0 & 1 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 & 0 & 1 \\
    0 & 1 & 0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 & 0 & 1 \\
  \end{bmatrix}
\end{align}

\begin{figure}
  \input{Pitfalls/multilayer.tex}
  \caption{Our three-state counter-example MP is extended to nine states to illustrate how the deadly triad problem could manifest in multi-layer neural networks. The self-loop in the original example is replaced with a clique with uniform transitions except as labelled with the original edge weight $e$. }
  \label{fig:multilayermdp}
\end{figure}

\subsubsection{Five-state}

We use this to generate Figure~\ref{fig:etagraph}. The transition matrix is:
\begin{align}
       & \begin{bmatrix}
           .4 & .4 & .2 & 0  & 0  \\
           .4 & .4 & .2 & 0  & 0  \\
           0  & .5 & 0  & .5 & 0  \\
           0  & 0  & .2 & .4 & .4 \\
           0  & 0  & .2 & .4 & .4 \\
         \end{bmatrix}
  \intertext{We set value function $V = [1, 1, 1.05, 1, 1]^\top$, $\gamma = 0.99$, $\epsilon=0.05$ and set basis:}
  \Phi & = \frac{1}{3}\begin{bmatrix}
                        3 & 0 & 0 \\
                        0 & 3 & 0 \\
                        1 & 1 & 1 \\
                        0 & 0 & 3 \\
                        1 & 1 & 1 \\
                      \end{bmatrix} *
  \begin{bmatrix}
    1 & 0    & 0   \\
    0 & 0.01 & 0   \\
    0 & 0    & 0.1 \\
  \end{bmatrix}
  \intertext{We also parameterize the off-policy distribution as:}
  D    & = \frac{1}{2}\text{diag}([p q, p q , 2(1 - p), p (1 - q), p (1 - q)])
\end{align}
where $p, q \in (0,1)$. We verify that $\sum D = 1$ over this domain. The on-policy distribution is $\pi=\sfrac{1}{12}[2,3,2,3,2]$. The plots correspond to the off-policy distributions:
\begin{enumerate}
  \item $p \to 0.77, q \to 0.85$
  \item $p \to 0.4, q \to 0.9$
  \item $p \to 0.02, q \to 0.2$
\end{enumerate}
