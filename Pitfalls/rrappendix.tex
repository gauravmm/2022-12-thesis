
\section{Appendix}

\subsection{Example Details}

\subsubsection{``Vacuous'' models}
\label{sec:choiceoffailure}
Without $\ell_2$ regularization, our linear model fails with asymptotic error. As this penalizes the $\ell_2$-norm of the learned weights, this removes the asymptote and so we can no longer use the existence of an asymptote as evidence of failure. Instead, we propose a different definition of failure by noting that, in the limiting case, regularization drives the learned weights to zero ($\lim_{\eta\to\infty} w^*(\eta) = \vec 0$). The learned value function $\Phi\cdot\vec 0 = \vec 0$ has no information about the true value function. We argue that if the error with any $\eta\in\mathbb R^+$ is never better than this case then the model is vacuous and hence adopt the threshold error of $\|\Phi\cdot\vec 0 - V\| = \|V\|$ to call a model vacuous. This explains the failure condition in Equation~\ref{eqn:vacuoustd}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applied to multi-layer networks}
\label{sec:multilayerapp}


\subsubsection{Relationship to modern RL algorithms}

It is still not obvious how strongly this instability affects modern RL algorithms, which are also sensitive to a variety of other failure modes. Unlike our analysis, the sampling distribution changes during training, and regularization mechanisms are more complex than simple $\ell_2$ penalities. The exact relationship between the instabilities we study and RL algorithms is an open problem, but we offer two pieces of indirect evidence suggesting there is a link.

First, in the offline/batch RL literature, it is well-known that online RL algorithms naively applied can catastrophically fail if the learned policy is not consistent with the data distribution. This is known as the distribution shift problem, \cite[p.~26]{levine2020offline} and offline RL algorithms are generally constructed to explicitly address this. Second, when using experience replay buffers in online RL algorithms, policy quality generally improves when older transitions are more quickly evicted \cite{fedus2020revisiting}. However, there are multiple factors at work here, and it is not possible to separate out the instability from off-policy sampling from the remaining factors.
