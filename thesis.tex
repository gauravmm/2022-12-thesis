\documentclass[12pt]{cmuthesis}
\usepackage[letterpaper,twoside,vscale=.7,hscale=.7,nomarginpar,hmarginratio=1:1]{geometry}

\draftstamp{\today}{DRAFT}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % micro typography
\usepackage{xcolor}         % colors
\usepackage{tikz}           % drawings
\usepackage{pgfplots}       % plots
\usepackage{xfrac}          % better fractions
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{comment,todonotes}
\usepackage{graphicx}
\usepackage[natbib]{biblatex}
\usepackage{fancyhdr,sectsty}
\usepackage{caption,subcaption}
\usepackage{minibox}
\usepackage{parskip,setspace}
\usepackage{epstopdf}
\usepackage[nottoc,numbib]{tocbibind} % Add bibliography to TOC
\usepackage[mathlines]{lineno}
\usepackage{tabularray}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{todonotes}
\usepackage{algorithm,algorithmic}
\usepackage{longtable}
\usepackage{import}
\usepackage{adjustbox}

\DeclareMathOperator*{\minimize}{minimize}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}


\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{matrix}
\usetikzlibrary{patterns}

\input{latexhacks/linenomath.tex}
\input{latexhacks/addplotgraphicsnatural.tex}
\input{latexhacks/mathstyle.tex}
\input{latexhacks/viridis.tex}
\input{latexhacks/math_commands.tex}

%\newfontfamily\OpenSans{Open Sans}[Scale=MatchLowercase]
%\newfontfamily\SourceSerifPro{Source Serif Pro}[Scale=MatchLowercase,Ligatures=TeX]newgeometry{left=3cm,bottom=0.1cm} where

\pgfplotsset{compat=1.18}
\usetikzlibrary{pgfplots.groupplots,positioning,fit,patterns,decorations.pathreplacing,external}
\usepackage{circuitikz}
\usepgfplotslibrary{fillbetween}

\tikzexternalize[prefix=tikz/]

% Margins
% \topmargin=-4pt
% \evensidemargin=0in
% \oddsidemargin=0in
% \textwidth=6.5in
% \textheight=8.5in
% \headsep=0.15in

\onehalfspacing

\title{ Stable Models and Temporal Difference Learning }
\author{ Gaurav Manek }
\date{\today}

\pagestyle{fancy}
% \fancyhead[L]{%
%   % thank egreg for his help in comment
%   \ifnum\value{chapter}=0\else\chaptername\ \thechapter\ --\ \fi\leftmark
% }
\fancyhead[EL]{\ifnum\value{chapter}=0\else\leftmark\fi}
\fancyhead[ER]{}
\fancyhead[OL]{}
\fancyhead[OR]{\rightmark}
\setlength{\headheight}{13.6pt}

\addbibresource{biblio.bib}

\begin{document}
\newgeometry{vscale=.8,hscale=.75}

\frontmatter
\pagestyle{empty}

\title{
	{\bf Stable Models and Temporal Difference Learning }}
\author{Gaurav Manek}
\date{March 2023}
\Year{2023}
\trnumber{CMU-CS-23-103}

\committee{
	J. Zico Kolter, Chair \\
	David Held \\
	Deepak Pathak \\
	Sergey Levine (Berkeley)
}

\support{This research was sponsored by A*STAR Graduate Academy, Robert Bosch GmbH, and the NSS Fellowship. }
\disclaimer{The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the U.S. government or any other entity. }
\keywords{Lyapunov Stability, Regularization, Deadly Triad, Offline Reinforcement Learning, Temporal Difference Learning, Reinforcement Learning, Neural Networks, Machine Learning Theory}

\maketitle

\cleardoublepage
\restoregeometry
\pagestyle{plain} % for toc, was empty


\begin{abstract}
	In this thesis, we investigate two different aspects of stability: the stability of neural network dynamics models and the stability of reinforcement learning algorithms. In the first chapter, we propose a new method for learning Lyapunov-stable dynamical models that is stable by construction, even when randomly initialized. We demonstrate the effectiveness of this method on damped multi-link pendulums and show how it can be used to generate high-fidelity video textures.

	In the second and third chapters, we focus on stability issues in reinforcement learning. In the second chapter, we demonstrate that regularization, a common approach to addressing instability in temporal difference (TD) learning, is not always effective. We show that TD learning can diverge even when regularization is used and demonstrate this phenomenon in standard examples as well as a novel problem we construct.

	In the third chapter, we propose a new resampling strategy called Projected Off-Policy TD (POP-TD), which resamples TD updates to come from a convex subset of ``safe'' distributions. Unlike existing resampling methods, it need not converge to the on-policy distribution. We show how this approach can mitigate the distribution shift problem in offline RL on a task designed to maximize such shift.

	Overall, this thesis advances novel methods for dynamics model stability and training stability in reinforcement learning, questions existing assumptions in the field, and points to promising directions for stability in model and reinforcement learning.
\end{abstract}



\clearpage

\tableofcontents

\cleardoublepage

\linenumbers

\chapter{Introduction}

In this thesis we examine two notions of stability: the predictions of neural network dynamics models and the training of reinforcement learning algorithms. The transition from the first notion of stability to the second is natural: the parameters of a stably trained model circumscribes, in parameter-space, a stable trajectory.
This relationship between stabilities has significant precedence in the foundational work of Temporal Difference (TD) learning theory \cite{tsitsiklis1996analysis}. % The goal for the work in this thesis was to relate these two stabilities, designing new RL algorithms with favorable stability properties.
% Instead, a major section of this thesis is devoted to demonstrating that the stability of TD learning is not as well understood as previously thought.

In the first chapter we propose a new method for learning Lyapunov-stable dynamical models and the certifying Lyapunov function in a fully end-to-end manner.
This works by carefully constructing a neural network to act as a Lyapunov function and learning a separate, unconstrained model. These two models are combined with a novel reprojection layer to produce models that are guaranteed stable by construction, even without any training. We show that such learning systems are able to model simple dynamical systems such as pendulums, and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion, given Temporal Difference (TD) data.

Next, we attempted to extend this work to learn control policies that produce stable trajectories. This is a natural extension of the previous work, equivalent to only minor changes to the construction of the dynamics model. Our work necessarily learned from TD data collected by a different policy (i.e. offline). Despite immense effort and many experiments, this technique never converged to reasonable solutions, even with regularization. This led to the insights in the second chapter.

TD is combined with function approximation (i.e. neural networks) and off-policy learning in modern Reinforcement Learning. However, these three ingredients are known as the \emph{deadly triad} \cite[p.~264]{sutton2020reinforcement}, because they are known to cause severe instability in the learning process \citet{tsitsiklis1996analysis}. While many variants of TD will provably converge despite the training instability, the quality of the solution at convergence is typically arbitrarily poor \citep{kolter2011fixed}. In the literature, there is a general belief that regularization can mitigate this instability, which is supported by basic analysis on the three standard examples.

However, this is not true! In the second chapter, we introduce a series of new counterexamples that are resistant to regularization. We demonstrate the existence of ``vacuous'' examples, which never do better than the limiting case regardless of the amount of regularization. This problem persists in most TD-based algorithms, which covers a wide swath of the RL literature; we make our analysis concrete by showing how this example forces the error bounds derived by \citet{zhang2021breaking} to permit vacuous solutions. We further demonstrate that regularization is not monotonic in TD contexts, and that it is possible for regularization to increase error (or cause divergence) around some critical values. We extend these examples to the neural network case, showing that these effects are not limited to the linear case and making the case for greater care in regularization in practical RL applications. Finally, contemporary versions of Emphatic-TD generally use a reversed version of TD to estimate the resampling function, which opens them up to instability from the same source as the original TD. We show that these techniques are similarly vulnerable. We show that regularization is not a panacea for stability in TD learning.


In the third chapter, we investigate new methods for stable TD learning that are resistant to off-policy divergence and that do not rely on regularization. Starting from previous work by \citet{kolter2011fixed}, we derive Projected Off-Policy TD, which reweighs TD updates to the closest distribution where the TD is non-expansive at the fixed point of its training. We learn the reweighing factors in-the-loop with vanilla TD methods (i.e. optimized as an augmented objective using stochastic gradient descent) and then apply those reweighing factors to each TD update. Crucially, this is distinct from contemporary work in the literature in that POP-TD does not resample to on-policy distribution, instead finding a ``safe'' distribution close to the data distribution. Applying this to a novel offline RL example, we can clearly demonstrate how POP-TD mitigates the \emph{distributional shift} between the dataset and the learned policy \cite{levine2020survey} while resampling as little as possible.

We conclude with a discussion on future directions that our work on stable models may take.


\mainmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Learning Stable Dynamics Models]{Learning Provably Stable Deep Dynamics Models}
\chaptermark{Stable Dynamics Models}
\input{LSDDM/main.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[The Pitfalls of Regularization in Off-Policy TD]{The Pitfalls of Regularization in Off-Policy Temporal Difference Learning}
\chaptermark{Regularization in Off-Policy TD}

\input{Pitfalls/rr.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Projected Off-Policy TD for Offline Reinforcement Learning}
\chaptermark{POP-TD for Offline RL}

\input{poptd/poptd.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}

We have examined two notions of stability with a subtle relationship: that of learned dynamics models, and the training of reinforcement learning algorithms. In doing so, we have introduced new techniques in both areas, as well as filled in a gap in the literature on the unsuitability of regularization to solve instability in RL.

One key gap in the RL literature that we hope to address in the future is how we should regularize deep RL in a principled manner. While our prior work shows that simple $\ell_2$ regularization can cause divergence, the literature is ripe for either adaptive regularization schemes that can detect and avoid pathological behavior, or for novel non-convex regularization that does not exhibit the same tendency to diverge.

Separately, there remains a large gap in a key area within offline RL in dealing with the distributional shift problem. While there have been many recent advances in the field, these advances have been largely incremental, and the field remains ripe for a novel perspective that can address this. We propose that POP-TD is that novel perspective. Unlike the existing literature, the key insight that POP-TD brings to the field is that we can resample to ``safe'' off-policy distributions that are close to the data distribution, instead of the on-policy distribution which may be arbitrarily far. With these novel POP techniques, we hope to allow offline RL to both (1) resample the data as little as possible thereby preserving the signal in the data, and (2) learn to generalize from a set of diverse and possibly even adversarial experts that complete tasks in mutually incompatible ways.

\appendix
\input{appendices/chapter.tex}

\backmatter{}
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography{}

\end{document}
