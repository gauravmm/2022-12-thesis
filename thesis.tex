\documentclass[11pt]{book}

\usepackage[letterpaper]{geometry}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % micro typography
\usepackage{xcolor}         % colors
\usepackage{tikz}           % drawings
\usepackage{pgfplots}       % plots
\usepackage{xfrac}          % better fractions
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{comment,todonotes}
\usepackage{graphicx}
\usepackage[natbib]{biblatex}
\usepackage{fancyhdr,sectsty}
\usepackage{caption,subcaption}
\usepackage{minibox}
\usepackage{parskip,setspace}
\usepackage{epstopdf}
\usepackage[nottoc,numbib]{tocbibind} % Add bibliography to TOC
\usepackage[mathlines]{lineno}
\usepackage{tabularray}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{todonotes}

\newenvironment{abstract}{}{}
\usepackage{abstract}

\DeclareMathOperator*{\minimize}{minimize}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}


\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{matrix}
\usetikzlibrary{patterns}

\input{latexhacks/linenomath.tex}
\input{latexhacks/addplotgraphicsnatural.tex}
\input{latexhacks/mathstyle.tex}
\input{latexhacks/viridis.tex}
\input{latexhacks/math_commands.tex}

%\newfontfamily\OpenSans{Open Sans}[Scale=MatchLowercase]
%\newfontfamily\SourceSerifPro{Source Serif Pro}[Scale=MatchLowercase,Ligatures=TeX]

%\bibliographystyle{abbrvnat}
\bibliography{biblio.bib}

\pgfplotsset{compat=1.18}
\usetikzlibrary{pgfplots.groupplots,positioning,fit,patterns,decorations.pathreplacing,external}
\usepackage{circuitikz}
\usepgfplotslibrary{fillbetween}

\tikzexternalize[prefix=tikz/]

% Margins
% \topmargin=-4pt
% \evensidemargin=0in
% \oddsidemargin=0in
% \textwidth=6.5in
% \textheight=8.5in
% \headsep=0.15in

\onehalfspacing

\title{ Temporal Difference Learning and Stable Models }
\author{ Gaurav Manek }
\date{\today}

\begin{document}
\frontmatter

\begin{titlepage}
	{
		\hfill\includegraphics[width=1.25in,trim=0 0 .125in .25in]{cmu/cmu-wordmark-square-w-on-r}
	}
	\begin{center}{%\SourceSerifPro
		\vfill

		{\Huge%\OpenSans
		{
		{Stable Models}\\[-.75em]
		\rule{0.25\textwidth}{1pt}
		\raisebox{-.1\baselineskip}{\textit{\Large and}}
		\rule{0.25\textwidth}{1pt}\\[.5em]
		{Temporal Difference Learning}
		}
		}\\


		\vspace{0.25in}

		\textbf{\large Gaurav Manek}\\

		\vfill

		Thesis

		\textit{In Partial Fulfillment of the Computer Science PhD Program}

		\vspace{0.33in}
		Computer Science Department\\
		School of Computer Science\\
		Carnegie Mellon University\\
		\today

		}\end{center}
\end{titlepage}

\cleardoublepage

\chapter{Abstract}

In this thesis, we investigate two different aspects of stability: the stability of neural network dynamics models and the stability of reinforcement learning algorithms. In the first chapter, we propose a new method for learning Lyapunov-stable dynamical models that is stable by construction, even when randomly initialized. We demonstrate the effectiveness of this method on damped multi-link pendulums and show how it can be used to generate high-fidelity video textures.

In the second and third chapters, we focus on stability issues in reinforcement learning. In the second chapter, we demonstrate that regularization, a common approach to addressing instability in temporal difference (TD) learning, is not always effective. We show that TD learning can diverge even when regularization is used and demonstrate this phenomenon in both standard examples and a novel problem we construct.

In the third chapter, we propose a new resampling strategy called Projected Off-Policy TD, which resamples TD updates to come from a convex subset of "safe" distributions. We show how this approach can mitigate the distribution shift problem in offline RL and improve the performance of Conservative Q-Learning on a variety of datasets.

Overall, this thesis advances novel methods for dynamics model stability and training stability in reinforcement learning, and points to promising directions for future research on stability in this field.


\clearpage

\tableofcontents

\cleardoublepage

\linenumbers

\chapter{Introduction}

In this thesis we examine two notions of stability: the predictions of neural network dynamics models and the training of reinforcement learning algorithms. The transition from the first notion of stability to the second is natural: the parameters of a stably trained model circumscribes, in parameter-space, a stable trajectory.
This relationship between stabilities has significant precedence in the foundational work of Temporal Difference (TD) learning theory \cite{tsitsiklis1996analysis}. The goal for the work in this thesis was to relate these two stabilities, designing new RL algorithms with favorable stability properties.
Unfortunately, this was not quite to be.

In the first chapter we propose a new method for learning Lyapunov-stable dynamical models and the certifying Lyapunov function in a fully end-to-end manner. 
This works by carefully constructing a neural network to act as a Lyapunov function and learning a separate, unconstrained model. These two models are combined with a novel reprojection layer to produce models that are guaranteed stable by construction, even without any training. We show that such learning systems are able to model simple dynamical systems such as pendulums, and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion, given Temporal Difference (TD) data.

Next, we attempted to extend this work to learn control policies that produce stable trajectories. This is a natural extension of the previous work, equivalent to only minor changes to the construction of the dynamics model. Our work necessarily learned from TD data collected by a different policy (i.e. offline). Despite immense effort and many experiments, this technique never converged to reasonable solutions, even with regularization. This led to the insights in the second chapter.

TD is combined with function approximation (i.e. neural networks) and off-policy learning in modern Reinforcement Learning. However, these three ingredients are known as the \emph{deadly triad} \cite[p.~264]{sutton2020reinforcement}, because they are known to cause severe instability in the learning process \citet{tsitsiklis1996analysis}. While many variants of TD will provably converge despite the training instability, the quality of the solution at convergence is typically arbitrarily poor \citep{kolter2011fixed}. In the literature, there is a general belief that regularization can mitigate this instability, which is supported by basic analysis on the three standard examples.

However, this is not true! In the second chapter, we introduce a series of new counterexamples that are resistant to regularization. We demonstrate the existence of ``vacuous'' examples, which never do better than the limiting case regardless of the amount of regularization. This problem persists in most TD-based algorithms, which covers a wide swath of the RL literature; we make our analysis concrete by showing how this example forces the error bounds derived by \citet{zhang2021breaking} to permit vacuous solutions. We further demonstrate that regularization is not monotonic in TD contexts, and that it is possible for regularization to increase error (or cause divergence) around some critical values. We extend these examples to the neural network case, and to Emphatic-TD to show that regularization is not a panacea for stability in TD learning.

In the third chapter, we investigate new methods for stable TD learning that are resistant to off-policy divergence and that do not rely on regularization. Starting from previous work by \citet{kolter2011fixed}, we derive Projected Off-Policy TD, which reweighs TD updates to the closest distribution where the TD is non-expansive at the fixed point of its training. We learn the reweighing factors in a manner similar to TD (i.e. optimized using stochastic gradient descent) and then applies those reweighing factors to each TD update. Applying this to offline RL, we can clearly demonstrate how POP-TD mitigates the \emph{distributional shift} between the dataset and the learned policy \cite{levine2020survey}.

We conclude with a discussion on future directions that our work on stable models may take.


\mainmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Learning Stable Dynamics Models]{Learning Provably Stable Deep Dynamics Models}
\chaptermark{Stable Dynamics Models}
\input{LSDDM/main.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[The Pitfalls of Regularization in Off-Policy TD]{The Pitfalls of Regularization in Off-Policy Temporal Difference Learning}
\chaptermark{Regularization in Off-Policy TD}

\input{Pitfalls/rr.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Projected Off-Policy TD for Offline Reinforcement Learning}
\chaptermark{POP-TD for Offline RL}

\input{poptd/poptd.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}

In prior work, we have identified a gap in the literature: that the common use of regularization to prevent instability from off-policy TD is not guaranteed to help and may even be harmful. Hence we propose POP-TD, a new algorithm that resamples TD to be consistent with a distribution under which TD updates are non-expansive (at convergence). We show this algorithm is successful on toy examples, and propose that we use it as a basis for a new algorithm for offline/batch RL.  The remaining work involves designing and proving this algorithm, validating structural assumptions made in the design of POP-TD, and potentially proving convergence and providing error bounds for POP-TD.

\appendix
\input{appendices/chapter.tex}

\backmatter{}
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography{}

\end{document}
