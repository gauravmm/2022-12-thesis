\documentclass[12pt]{cmuthesis}
\usepackage[letterpaper,twoside,vscale=.7,hscale=.7,nomarginpar,hmarginratio=1:1]{geometry}

% \draftstamp{\today}{DRAFT}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % micro typography
\usepackage{xcolor}         % colors
\usepackage{tikz}           % drawings
\usepackage{pgfplots}       % plots
\usepackage{xfrac}          % better fractions
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{comment,todonotes}
\usepackage{graphicx}
\usepackage[natbib]{biblatex}
\usepackage{fancyhdr,sectsty}
\usepackage{caption,subcaption}
\usepackage{minibox}
\usepackage{parskip,setspace}
\usepackage{epstopdf}
\usepackage[nottoc,numbib]{tocbibind} % Add bibliography to TOC
\usepackage[mathlines]{lineno}
\usepackage{tabularray}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{todonotes}
\usepackage{algorithm,algorithmic}
\usepackage{longtable}
\usepackage{import}
\usepackage{adjustbox}
\usepackage{epstopdf}

\DeclareMathOperator*{\minimize}{minimize}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}


\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{matrix}
\usetikzlibrary{patterns}

\input{latexhacks/linenomath.tex}
\input{latexhacks/addplotgraphicsnatural.tex}
\input{latexhacks/mathstyle.tex}
\input{latexhacks/viridis.tex}
\input{latexhacks/math_commands.tex}

%\newfontfamily\OpenSans{Open Sans}[Scale=MatchLowercase]
%\newfontfamily\SourceSerifPro{Source Serif Pro}[Scale=MatchLowercase,Ligatures=TeX]newgeometry{left=3cm,bottom=0.1cm} where

\pgfplotsset{compat=1.18}
\usetikzlibrary{pgfplots.groupplots,positioning,fit,patterns,decorations.pathreplacing,external}
\usepackage{circuitikz}
\usepgfplotslibrary{fillbetween}

\tikzexternalize[prefix=tikz/]

% Margins
% \topmargin=-4pt
% \evensidemargin=0in
% \oddsidemargin=0in
% \textwidth=6.5in
% \textheight=8.5in
% \headsep=0.15in

\onehalfspacing

\title{ Stable Models and Temporal Difference Learning }
\author{ Gaurav Manek }
\date{\today}

\pagestyle{fancy}
% \fancyhead[L]{%
%   % thank egreg for his help in comment
%   \ifnum\value{chapter}=0\else\chaptername\ \thechapter\ --\ \fi\leftmark
% }
\fancyhead[EL]{\ifnum\value{chapter}=0\else\leftmark\fi}
\fancyhead[ER]{}
\fancyhead[OL]{}
\fancyhead[OR]{\rightmark}
\setlength{\headheight}{13.6pt}

\addbibresource{biblio.bib}

\begin{document}
\newgeometry{vscale=.8,hscale=.75}

\frontmatter
\pagestyle{empty}

\title{
	{\bf Stable Models and \\ Temporal Difference Learning }}
\author{{\bf Gaurav Manek}}
\date{April 2023}
\Year{2023}
\trnumber{CMU-CS-23-103}

\committee{
	J. Zico Kolter, Chair \\
	David Held \\
	Deepak Pathak \\
	Sergey Levine (UC Berkeley)
}

\support{This research was sponsored by Agency for Science, Technology and Research (A*STAR) Singapore and Robert Bosch GmbH }
\disclaimer{The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the U.S. government or any other entity. }
\keywords{Lyapunov Stability, Regularization, Deadly Triad, Offline Reinforcement Learning, Temporal Difference Learning, Reinforcement Learning, Neural Networks, Machine Learning, Artificial Intelligence}

\maketitle

\cleardoublepage
\restoregeometry
\pagestyle{plain} % for toc, was empty


\begin{abstract}
	In this thesis, we investigate two different aspects of stability: the stability of neural network dynamics models and the stability of reinforcement learning algorithms. In the first chapter, we propose a new method for learning Lyapunov-stable dynamics models that are stable by construction, even when randomly initialized. We demonstrate the effectiveness of this method on damped multi-link pendulums and show how it can be used to generate high-fidelity video textures.

	In the second and third chapters, we focus on the stability of Reinforcement Learning (RL). In the second chapter, we demonstrate that regularization, a common approach to addressing instability, behaves counterintuitively in RL settings. Not only is it sometimes ineffective, but it can also cause instability. We demonstrate this phenomenon in both linear and neural network settings. Further, standard importance sampling methods are also vulnerable to this.

	In the third chapter, we propose a mechanism to stabilize off-policy RL through resampling. Called Projected Off-Policy TD (POP-TD), it resamples TD updates to come from a convex subset of ``safe'' distributions instead of (as in other resampling methods) resampling to the on-policy distribution. We show how this approach can mitigate the distribution shift problem in offline RL on a task designed to maximize such shift.

	Overall, this thesis advances novel methods for dynamics model stability and training stability in reinforcement learning, questions existing assumptions in the field, and points to promising directions for stability in model and reinforcement learning.
\end{abstract}



\cleardoublepage

\begin{acknowledgments}
	Although it is not possible to express my gratitude to everyone who has played a role in my academic and personal growth, I want to acknowledge and appreciate the countless friends, family, and collaborators whose contributions have been invaluable to me.

	First and foremost, my advisor Zico Kolter. His guidance, advice, enthusiasm, expertise, encouragement, and unwavering support have been invaluable to me throughout this journey. His involvement has helped me develop a deeper understanding of the subject matter and how to approach research in a meaningful way. I also wish to thank Sergey Levine, David Held, and Deepak Pathak, for their invaluable feedback, suggestions, and guidance as members of my thesis committee.

	I would like to express my sincere appreciation to my collaborators on the POP-TD project Melrose Roderick and Felix Berkenkamp, for their insight, good humor, time, patience, and tenacity.

	From the department, I wish to thank Ruben Martins for engaging with my fledgeling ideas in his field. Also, the support staff who make the department what it is: Catherine Copetas, Deb Cavlovich, Ann Stetser, Charlotte Yano, and the many, many others. I also owe a debt of gratitude to Francisco Maturana, my roommate of six years, colleague in the CS department, and friend.

	\clearpage

	From A*STAR, I wish to thank the many people in the past and present who have molded me as researchers and supported me through my PhD, especially Vijay Chandrasekhar and Li Xiaoli. I also wish to thank the many senior researchers who have invested their time in me, and the dedicated scholarship officers who have supported me for a decade. From my undergraduate years in Brown I need to thank Shriram Krishnamurthi and Stefanie Tellex for starting me down this journey.

	And most of all, I would like to express my heartfelt thanks to my parents Mukesh and Sonal Manek, my brother Gautam Manek, and my extended family for their unwavering love, encouragement, and support throughout the highs and lows of my academic journey. Their steadfast presence, undying love, and unwavering support have been the foundation upon which I have built my academic success, and I am forever indebted to them.
\end{acknowledgments}

\cleardoublepage

\tableofcontents

\cleardoublepage

\linenumbers

\chapter{Introduction}

In this thesis we examine two notions of stability: that of neural network dynamics models and the training of reinforcement learning algorithms. There is a natural transition from the first notion of stability to the second is natural: the parameters of a stably trained model circumscribes, in parameter-space, a stable trajectory.
This relationship between stabilities has significant precedence in the foundational work of Temporal Difference (TD) learning theory \cite{tsitsiklis1996analysis}.

In the first chapter we propose a new method for learning Lyapunov-stable dynamical models and the certifying Lyapunov function in a fully end-to-end manner. Instead of enforcing stability by some loss function, we guarantee stability everywhere by construction.
This works by carefully constructing a neural network to act as a Lyapunov function, learning a separate, unconstrained dynamics model, and then combining these two models with a novel reprojection layer. This produces models that are guaranteed stable by construction everywhere in the state space, even without any training. We show that such learning systems are able to model simple dynamical systems such as pendulums, and can be combined with additional deep generative models to learn complex dynamics, such as video textures, in a fully end-to-end fashion.

In modern Reinforcement Learning, TD is combined with function approximation (i.e. neural networks) and off-policy learning. However, these three are known as the \emph{deadly triad} \cite[p.~264]{sutton2020reinforcement}, because they may cause severe instability in the learning process \citet{tsitsiklis1996analysis}. While many variants of TD will provably converge despite the training instability, the quality of the solution at convergence is typically arbitrarily poor \citep{kolter2011fixed}. In the literature, there is a general belief that regularization can mitigate this instability, which is supported by basic analysis on the three standard examples.

However, this is not true! In the second chapter, we introduce a series of new counterexamples that are resistant to regularization. We demonstrate the existence of ``vacuous'' examples, which never do better than the limiting case regardless of the amount of regularization. This problem persists in most TD-based algorithms, which covers a wide swath of the RL literature; we make our analysis concrete by showing how this example forces the error bounds derived by \citet{zhang2021breaking} to be extremely loose in practice. We further demonstrate that regularization is not monotonic in TD contexts, and that it is possible for regularization to increase error (or cause divergence) around some critical values. We extend these examples to the neural network case showing that these effects are not limited to the linear case and making the case for greater care in regularization in practical RL applications. Finally, there is a line of work starting with Emphatic-TD which seeks to stabilize off-policy training by resampling TD updates to appear on-policy. Contemporary Emphatic algortihms generally use a reversed version of TD to estimate the resampling function, which opens them up to instability from the same source as the original TD. We show that these techniques are similarly vulnerable. We show that regularization is not a panacea for stability in TD learning.

In the third chapter, we investigate new methods for stable TD learning that are resistant to off-policy divergence. Starting from an idea introduced by \citet{kolter2011fixed} we derive Projected Off-Policy TD, which reweighs TD updates to the closest distribution where the TD is non-expansive at the fixed point of its training. We learn the reweighing factors in the training loop using stochastic gradient descent (i.e. with time- and space-complexity comparable to learning the value function) and then apply those reweighing factors to each TD update. Crucially, this is distinct from contemporary work in the literature in that POP-TD does not resample to on-policy distribution, instead finding a ``safe'' distribution close to the data distribution. Applying this to a novel offline RL example, we can clearly demonstrate how POP-TD mitigates the \emph{distributional shift} between the dataset and the learned policy \cite{levine2020survey} while resampling as little as possible.

We conclude with a discussion on future directions that our work on stable models may take.


\mainmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Learning Stable Dynamics Models]{Learning Provably Stable Deep Dynamics Models}
\chaptermark{Stable Dynamics Models}
\input{LSDDM/main.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[The Pitfalls of Regularization in Off-Policy TD]{The Pitfalls of Regularization in Off-Policy TD Learning}
\chaptermark{Regularization in Off-Policy TD}

\input{Pitfalls/rr.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Projected Off-Policy TD for Offline Reinforcement Learning}
\chaptermark{POP-TD for Offline RL}

\input{poptd/poptd.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter{}
\chapter{Conclusion}

We have examined two notions of stability with a subtle relationship: that of learned dynamics models, and the training of reinforcement learning algorithms. In doing so, we have introduced new techniques in both areas, as well as filled in a gap in the literature on the unsuitability of regularization to solve instability in RL.

One key gap in the RL literature that we hope to address in the future is how we should regularize deep RL in a principled manner. While our prior work shows that simple $\ell_2$ regularization can cause divergence, the literature is ripe for either adaptive regularization schemes that can detect and avoid pathological behavior, or for novel non-convex regularizations that fail similarly.

Separately, there remains a large gap in a key area within offline RL in dealing with the distributional shift problem. While there have been many recent advances in the field, these advances have been largely incremental, and the field remains ripe for a novel perspective that can address this. We propose that POP-TD is that novel perspective. Unlike the existing literature, the key insight that POP-TD brings to the field is that we can resample to ``safe'' off-policy distributions that are close to the data distribution, instead of the on-policy distribution which may be arbitrarily far. With these novel POP techniques, we hope to allow offline RL to resample the data as little as possible to avoid instability from large resampling coefficients, and learn to generalize from a set of diverse and possibly even adversarial experts that complete tasks in mutually incompatible ways.

\appendix
\input{appendices/chapter.tex}

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography{}

\end{document}
