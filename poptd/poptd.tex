\input{poptd/colorglyphs.tex}

A key problem in offline Reinforcement Learning (RL) is the mismatch between the dataset and the distribution over states and actions visited by the learned policy, called the \emph{distribution shift}. This is typically addressed by constraining the learned policy to be close to the data generating policy, at the cost of performance of the learned policy. We propose Projected Off-Policy TD (POP-TD), a new critic update rule that resamples TD updates to allow the learned policy to be distant from the data policy without catastrophic divergence. We show how various offline RL algorithms, notably Conservative Q-Learning (CQL), can be augmented to mitigate distribution shift. We evaluate our algorithm on some toy and small test cases, and set the stage for future work in this area.

\emph{From as-yet unpublished work by \citeauthor{manek2023poptd} (\citeyear{manek2023poptd})}

\clearpage


\section{Introduction}

Reinforcement Learning (RL) aims to learn policies that maximize rewards in Markov Decision Processes (MDPs) through interaction, generally using Temporal Difference (TD) methods. In contrast, offline RL focuses on learning optimal policies from a static dataset sampled from an unknown policy, possibly a policy designed for a different task. Thus, algorithms are expected to learn without the ability to interact with the environment.
This is useful in environments that are expensive to explore (such as running a Tokamak nuclear reactor \cite{degrave2022magnetic}), or high-dimensional environments with cheap access to expert or near-expert trajectories (such as video games). \citet{levine2020survey} present a comprehensive survey of the area.

Since in offline RL the data is gathered before training begins, there is a mismatch between the the state-distributions implied by the learned policy and the data.
When applying naive RL algorithms in this setting, they tend to bootstrap from regions with little or no data, causing runaway self-reinforcement.S
Offline RL algorithms like Conservative Q-Learning (CQL) \cite{kumar2020cql}, on the other hand, generally constrain the learned policy to remain within the support of the data. While this works well in practice, there still remains a large gap in performance between online and offline RL. One reason for this is an additional subtlety to distribution shift: because of the combination of off-policy RL and function approximation, it is possible for RL to diverge if the generating policy and the learned policy are sufficiently different.

\begin{figure}[t]
  \centering
  \input{poptd/simpleoffpolicy/mapillustr.tex}
  \caption{A simple grid environment illustrating distribution shift despite complete support. We wish to learn the optimal trajectory ({\color{optimpolicy} \optimpolicyglyph}) from a suboptimal data policy ({\color{datapolicy} \datapolicyglyph}) which is $\epsilon$-dithered to get sufficient coverage. When we apply Q-learning methods to this, training often diverges to arbitrarily poor values. This is a consequence of distribution shift. In this paper, we propose a technique to solve this divergence. }
  \label{fig:sopmap}
\end{figure}

We illustrate a simple case in Figure~\ref{fig:sopmap}, where a simple grid environment is designed to elicit the shortest trajectory from start (S) to goal (G). Agents can move one step in each cardinal direction, reaching the goal yields a unit reward, and the episode ends on reaching the goal or any marked cell (X). We generate a dataset by following a suboptimal data policy ({\color{datapolicy} \datapolicyglyph}) with sufficient dithering to guarantee that every state-action pair is represented. If we use a tabular Q-function, we can recover the optimal policy ({\color{optimpolicy} \optimpolicyglyph}) and obtain the true value function. When we use a linear Q-function, however, the error is much larger. We find that about half of random initializations lead to Q-functions that either diverge or converge to large error. This shows how even with full coverage of states and actions, distribution shift can be a significant source of error. We provide more details in Section~\ref{sec:expsimpleoffpolicy}.

\paragraph{Contributions}
In this chapter, we introduce POP-TD, a novel method of mitigating the error from off-policy learning. We show theoretically that this method bounds the off-policy approximation error for TD-based RL methods. Finally, we illustrate empirically that POP-TD can improve performance on offline RL tasks.
%// TODO: Revisit after experiments are complete.

\section{Related Work}

\paragraph{Off-Policy TD Learning} Instability from learning off-policy has also been studied in the classic RL literature.
First described by \citet{tsitsiklis1996analysis}, the use of TD learning, function approximation, and off-policy sampling may cause severe instability or divergence. This is known as the \emph{deadly triad} \citep[p.~264]{sutton2020reinforcement} and even if many variants of TD still converge, the quality of the solution at convergence may be arbitrarily poor \citep{kolter2011fixed}.


There are three existing lines of work in the literature that attempt to resolve this: regularization, Emphatic reweighing, and TD Distribution Optimization (TD-DO).
The first attempts to regularize TD, typically with $\mathcal L_2$-norm weight regularization. Alternative regularization schemes are $\mathcal L_1$ \citep{mahadevan2014proximal}, convex \citep{yu2017convergence}, and bounds propagation \citep{kumar2020discor}. There are well-documented failure modes related to regularization \cite{manek2022pitfalls}.
The second line started with Emphatic-TD, in which \citet{sutton2016emphatic} note that it is possible to reweigh samples obtained off-policy so they appear to be on-policy. Such methods learn the follow-on trace using Monte-Carlo methods (in the original), TD \citep{jiang2021learning,zhang2020provably} or techniques similar to TD \citep{hasselt2021expected}. The third method, TD-DO, works by solving a small optimization problem on each TD update to reweigh samples to satisfy the Non-Expansion Criterion, which we introduce in the next section.

\paragraph{Off-Policy and Offline Deep RL}

Nearly all modern TD-based deep RL methods perform off-policy learning in practice.
To improve data efficiency and learning stability, an experience replay buffer is often used. This buffer stores samples from an outdated version of the policy \citep{mnih2015humanlevel}.
Additionally, exploration policies, such as a epsilon greedy \citep[p.~100]{sutton2020reinforcement} or Soft Actor Critic (SAC)-style entropy regularization \citep{haarnoja2018soft} \footnote{While the original SAC algorithm is technically on-policy since it learns an entropy-regularized value function, the entropy-regularization is often dropped from the value-function estimate in practice to improve performance.}, are often used, which also results in off-policy learning.
In practice, the difference between the current policy and the samples in the buffer is limited by setting a limit to the buffer size and discarding old data; or by keeping the exploration policy relatively close to the learned policy. In practice, this is sufficient to prevent outright divergence, though the extent to which it decreases performance is not well-understood.

However, in the offline RL setting where training data is static, there is usually a much larger discrepancy between the state-action distribution of the data and the distribution induced by the learned policy.
This discrepancy presents a significant challenge for offline RL \citep{levine2020survey}.
While this distributional discrepancy is often presented as a single challenge for offline RL algorithms, there are two distinct aspects of this challenge that can be addressed independently: \textit{support mismatch} and \textit{proportional mismatch}.
When the support of the two distributions differ, learned value functions will have arbitrarily high errors in low-data regions.
Support mismatch is dealt with by either constraining the KL-divergence between the data and learned policies \citep{fujimoto2019off, kumar2019stabilizing, wu2019behavior}, by penalizing or pruning low-support (or high-uncertainty) actions \citep{kumar2020cql, yu2020mopo, kidambi2020morel}.

Even when the support of the data distribution matches that of the policy distribution, naive TD methods can produce unbounded errors in the value function \citep{tsitsiklis1996analysis}.
We call this challenge \textit{proportional mismatch}.

Importance sampling (IS) \citep{precup2000eligibility} is one of the most widely used techniques to address proportional mismatch.
The idea with IS is to compute the differences between the data and policy distributions for every state-action pair and re-weight the TD updates accordingly.
However, these methods suffer from variance that grows exponentially in the trajectory length.
Several methods have been proposed to mitigate this challenge and improve performance of IS in practice \citep{hallak2017consistent, gelada2019off, nachum2019dualdice, nachum2019algaedice, liu2018breaking}, but the learning is still far less stable than other offline deep RL methods.
In this work, we propose a new method to bound the value-function approximation errors caused by proportional mismatch without the need to explicitly compute (or approximate) IS weights.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Setting and Notation }

Consider the $n$-state Markov chain $(\mathcal S, P, R, \gamma)$, with state space $\mathcal S$, transition function $P : \mathcal{S} \times \mathcal{S} \to \mathbb{R}_+$, reward function $R : \mathcal S \to \mathbb R$, and discount factor $\gamma \in [0, 1]$.
% $P \in \mathbb R^{n\times n}$ is the transition matrix, with $P_{ij}$ encoding the probability of moving from state $i$ to $j$.
Because the state-space is finite, it can be indexed as $\mathcal{S} = \{1, \ldots, n\}$.
This allows us to use matrix rather than operator notation.
The expected $\gamma$-discounted future reward of being in each state $V(s) \coloneqq \E \left[\left.\sum_{t=0}^\infty \gamma^t R(s_t) \right| s_0 = s \right]$ is called the value function.
The value function is consistent with Bellman's equation (in matrix form):
\begin{align}
  V = R + \gamma P V .
\end{align}
In the linear setting, we approximate the value function as $V(s) \approx w^\top \phi(s)$, where $\phi : \mathcal S \to \mathbb R^{k}$ is a fixed basis function and we estimate parameters $w \in \mathbb R^k$. In matrix notation, we write this as $V \approx \Phi w$.

In this work, we are interested in the offline learning setting, where the sampling distribution $\mu$ differs from the stationary distribution $\nu$.
In this setting, the TD solution is:
\begin{align}
  \Phi w & = \Pi_{\mu} (R + \gamma P \Phi w) ,
\end{align}
where $\Pi_\mu = \Phi(\Phi^\top D_{\mu} \Phi)^{-1}\Phi^\top D_{\mu}$ is the projection onto the column space of $\Phi$ weighted by the data distribution $\mu$ through the matrix $D_{\mu} = \diag(\mu)$. This projection may be arbitrarily far from the true solution, and so the error may be correspondingly large. The literature bounds the error as:
\begin{theorem}
  The error at the TD fixed point is $\|\Phi w - V \|_{D_\mu}$. Lemma 6 from \cite{tsitsiklis1996analysis} bounds this in terms of error projecting $V$ onto the column space of $\Phi$:
  \begin{align}
    \|\Phi w - V \|_{D_\mu} & \leq \frac{1}{1-\gamma} ~ \| \Pi_{\mu} V - V \|_{D_\mu}
  \end{align} \label{thm:boundTDError}
\end{theorem}

\subsection{Kolter's Non-Expansion Criterion (KNEC)}
Thus far we have left open the notion of a ``safe'' distribution to resample TD updates to. The on-policy distribution must be safe, but we need to establish a criteria for acceptable off-policy distributions. \citeauthor{tsitsiklis1996analysis} lay the groundwork for this by analyzing the training of on-policy TD as a dynamical system and showing that once TD reaches its fixed point, subsequent TD updates form a non-expansive mapping around that fixed point (\citeyear[lemma 4]{tsitsiklis1996analysis}), and therefore prove that on-policy TD does not diverge.

To do this, they begin with the fact that error bounds from on-policy TD follow the property that the $D-$norm of any vector $x \in \mathbb R^n$ is non-expansive through the transition matrix. That is: $\|Px\|_D \leq \|x\|_D$, where $D = \diag(\pi)$. \citet{kolter2011fixed} extend this analysis to the off-policy case, deriving a linear matrix inequality (LMI) under which the TD updates are guaranteed to be non-expansive around the fixed point. This is Kolter's Non-Expansion Criterion (\citeyear{kolter2011fixed}):
\begin{align}
  \|\Phi w - V\|_D    & \leq \frac{1 + \gamma \kappa(D^{-\sfrac{1}{2}}D^{\sfrac{1}{2}})}{1 - \gamma} \|\Pi_D V - V\|_D
  \intertext{From this bound, he derives \emph{Kolter's non-expansion condition}:}
  \|\Pi_D P\Phi w\|_D & \leq \|\Phi w\|_D \qquad (\forall w \in \mathbb R^n) \label{eqn:kolterthm2bound}
  \intertext{This holds if and only if the matrix $F_D$ is positive semi-definite}
  F_D                 & \equiv \begin{bmatrix}
                                 \Phi^\top D \Phi        & \Phi^\top D P \Phi \\
                                 \Phi^\top P^\top D \Phi & \Phi^\top D \Phi
                               \end{bmatrix} \succcurlyeq 0
  \intertext{Equivalently, in terms of the expectation over states:}
  \\  \E_{s\sim \mu, s'\sim p(\cdot|s)} & \left[\begin{bmatrix}
      \phi(s)\phi(s)^\top  & \phi(s)\phi(s')^\top \\
      \phi(s')\phi(s)^\top & \phi(s)\phi(s)^\top
    \end{bmatrix}\right] \succcurlyeq 0 . \label{eqn:knec}
\end{align}
This constraint describes a convex subset of $D$. As a $2k\times 2k$ matrix (where $k$ is the number of features), $F$ is prohibitively large to enumerate for any real RL problem, and so our algorithm is designed to make use of this without ever constructing it directly. Further, we notice that the construction of $F_D$ depends on $P$, the transition matrix of the underlying Markov process, which complicates how we construct it from samples.

For convenience, we write this as:
\begin{align}
   & \E_{s\sim q}  [F(s)] \succcurlyeq 0 \text{, where}                            \\
   & F(s) = \E_{s'\sim p(s'|s)}  \left[\begin{bmatrix}
                                           \phi(s)\phi(s)^\top  & \phi(s)\phi(s')^\top \\
                                           \phi(s')\phi(s)^\top & \phi(s)\phi(s)^\top
                                         \end{bmatrix}\right] .
\end{align}
KNEC is an expectation over some state distribution $q$ and transition distribution $p(s,s') = p(s'|s) \mu(s)$. Because it is an LMI, the satisfying state distributions $q$ form a convex subset.

Directly constructing $F(s)$ or $F(s, s')$ is impossible on all but the simplest examples -- it would take $\mathcal O(k^2n)$ or $\mathcal O(k^2n^2)$ memory to hold all the necessary data. Instead we exploit the structure inherent in the problem to make use of $F(s)$ without creating it.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Projected Off-Policy TD (POP-TD) }

We propose an alternative approach to stabilizing off-policy training, based on Kolter's Non-Expansion Criterion \citep{kolter2011fixed}. POP-TD identifies a convex set of ``safe'' distributions that satisfy KNEC and reweighs TD updates to come from that set. In contrast to TD-DO, POP-TD uses solves a different optimization problem using a two-timescales update with fixed cost per iteration, allowing it to scale to real-world problems.

We begin by deriving the projected off-policy update for Markov Chains, without a separate policy function. We will extend this derivation to support actions and Markov Decision Processes (MDPs) in Section~\ref{sec:popq}. Our algorithm resamples TD updates so they come from some distribution $q$ for which KNEC holds. Given input data $(x_1, x_2, \ldots)$, this is the same as finding a set of weights $q_1, q_2, \ldots$ such that
\begin{align}
  \sum_i q_i & \cdot F(x_i) \succcurlyeq 0
\end{align}
% We also need to constrain our choice of distribution $q$ to ensure the quality of the solution.


\subsection{I- and M-projections} \label{sec:improj}
The Kullback-Leibler divergence is an \emph{asymmetric} measure, and so it is usually the case that $\min_q~\text{KL}(q||\mu) \neq \min_q~\text{KL}(\mu||q)$. The former (``from $\mu$ to $q$'') is an information (or I-)projection, which tends to under-estimate the support of $q$ potentially excluding possible sampling distributions to reweigh to. The latter (``from $q$ to $\mu$'') is a moment (or M-)projection, which tends to over-estimate the support of $q$ and avoid zero solutions. In our solution, we are proposing using an I-projection instead of the M-projection used by \citet{kolter2011fixed}.


\subsection{Optimizing the distribution}
\label{sec:distriboptim}

In the previous section we have characterized a convex subset of off-policy distributions under which TD learning is guaranteed not to diverge. If we can discover any such distribution for a particular TD problem, we can reweigh our TD updates (from any distribution) so they appear consistent with this reweighing distribution. This is related to the main insight in Emphatic-TD \citep{sutton2016emphatic}, with the key innovation that we can take any non-expansive distribution \emph{not just the on-policy distribution}.

We can now write down the optimization problem that we wish to solve:
\begin{align}
  \underset{q}{\text{minimize}}~\text{KL}(q||\mu) & \qquad \text{s.t. } ~ E_{s\sim q}[F(s)] \succcurlyeq 0
\end{align}
We are searching for $q$, the closest distribution to the sampling distribution $\mu$ such that $F$ is PSD under $q$. Note that we could in principle minimize any notion of ``closest'' to find some satisfying distribution -- for example \citet{kolter2011fixed} explores the effects of minimizing $\text{KL}(\mu||q)$.

We construct the dual of this problem:
\begin{align}
  \underset{Z\succcurlyeq 0}{\text{maximize}}~ \underset{q}{\text{minimize}}~\text{KL}(q||\mu) - \tr Z^\top \E_{s\sim q}[F(s)]
\end{align}
Using the Lagrange multiplier $Z\in\mathbb R^{2k\times 2k}$, we solve the inner optimization problem:
\begin{align}
  \underset{q}{\text{minimize}} - H(q) - \E_{s\sim q}[\log \mu(s) + \tr Z^\top F(s)]
\end{align}
Writing down Lagrangian and solving for the optima, we obtain:
\begin{align}
  q^*(s) & \propto \mu(s)\exp( \tr Z^\top F(s))
\end{align}
(Subject to the constraint that $q^*(s)$ is normalized so it must sum to 1 over all $s$.)

Plugging this back into our dual formulation, we obtain the optimization problem:
\begin{align}
  \underset{Z\succcurlyeq 0}{\text{maximize}}   & -\log \E_{s\sim\mu} [ \exp(\tr Z^\top F(s)) ]
  \intertext{Which we can simplify to}
  \underset{Z\succcurlyeq 0}{\text{minimize}} ~ & \E_{s\sim\mu} [ \exp(\tr Z^\top F(s)) ]
\end{align}

As discussed earlier, $F(s)$ cannot be directly constructed; instead, we assume that $Z$ holds a specific structure and optimize the problem.


\subsection{The structure of Z}

Our next goal is to transform this constrained optimization problem into an unconstrained problem over a low-rank version of $Z$, suitable for learning via SGD.

We assume (and later check!) that the solution for $Z$ is low-rank. Intuitively, this is because $\E_{s\sim\mu}[F(s)]$ is PSD when $\mu$ is close to $\pi$, and for most MDPs, sampling off-policy leads to only a small number of negative eigenvalues that need to be corrected by $Z$. \citet{kolter2011fixed} provides a technical explanation: by the KKT conditions, $Z$ will have rank complementary to $\E_{s\sim\mu}[F(s)]$, and the latter is expected to be full rank. It is worth noting that this ``almost-PSD'' assumption is common in the field.

We make the strong assumption that $Z$ has rank $m$, where $m << k$. We apply the Burer-Montiero approach \citep{burer2003nonlinear} to convert the constrained optimization problem over $Z$ into an unconstrained optimization over matrices $A\in \mathbb R^{k\times m}$ and $B\in \mathbb R^{k\times m}$:
\begin{align}
  Z^\star & = \begin{bmatrix} A \\ B \end{bmatrix} \begin{bmatrix} A \\ B\end{bmatrix}^T
\end{align}
This allows us to represent the PSD matrix with controllable rank. Substituting this into the dual formulation, we get:
\begin{align}
  \underset{y}{\text{minimize}} & \;\; \mathbf{E}_{s\sim\mu} \left [ \exp\left (\tr  \begin{bmatrix} A \\ B \end{bmatrix} \begin{bmatrix} A \\ B \end{bmatrix}^T F(s) \right )  \right ]
\end{align}

We can leverage the structure of $F(s)$ to simplify the trace term:
\begin{align}
  \tr Z^T F(s)
   & \tr  \begin{bmatrix} A \\ B \end{bmatrix}^T \begin{bmatrix} A \\ B \end{bmatrix}^T F(s)
  \\ & = \begin{bmatrix} A \\ B \end{bmatrix}^T F(s) \begin{bmatrix} A \\ B \end{bmatrix}
  \\ & = \begin{bmatrix} A \\ B \end{bmatrix}^T  \mathbf{E}_{s' \sim p(s'|s)}  \left[ \begin{array}{cc} \phi(s)\phi(s)^T & \phi(s)\phi(s')^T \\ \phi(s')\phi(s)^T & \phi(s)\phi(s)^T \end{array} \right]
  \begin{bmatrix} A \\ B \end{bmatrix}
  \\  & =  (A + B)^T \phi(s)\phi(s)^T (A+B)
  - 2A^T \mathbf{E}_{s' \sim p(s'|s)} \left [\phi(s)(\phi(s)- \phi(s'))^T \right ] B
\end{align}


This allows us to rewrite the optimization problem as:
\begin{align}
  \underset{y}{\text{minimize}} & \;\; \mathbf{E}_{s\sim\mu} \left [ \exp\left((A + B)^T \phi(s)\phi(s)^T (A+B)
    - 2A^T \mathbf{E}_{s' \sim p(s'|s)} \left [\phi(s)(\phi(s)- \phi(s'))^T \right ] B
    \right)  \right ]
\end{align}
where the small parameters $A$ and $B$ can be optimized with regular gradient-descent methods.


\subsection{Update rules}

We can't directly optimize our problem because that would require us to estimate the inner expectation term. Instead, we use a two-timescales approach by estimating two (dependent) quantities separately and improving them at potentially different rates. This (with a little tuning) can generally converge to a valid solution.

We choose to estimate the matrices $A, B \in \mathbb R^{k\times m}$ and separately the function $g : \mathcal S \in \mathbb R$ where
\begin{align}
  g(s) & \approx (A + B)^T \phi(s)\phi(s)^T (A+B)
  - 2A^T \mathbf{E}_{s' \sim p(s'|s)} \left [\phi(s)(\phi(s)- \phi(s'))^T \right ] B
\end{align}
which can be approximated as a linear function (or a neural network). The size of the weights learned by POP-TD are therefore $\mathcal O(k)$, comparable to the size of vanilla Q-learning.

We find the derivative of the objective with respect to $g(s)$ and $y$ and use that to construct update rules for our algorithm.
\begin{align}
   & \nabla_y \mathbf{E}_\mu \left [ \exp (2 y^T A(s) y) \right ]
  \\ & =
  \mathbf{E}_{s \sim \mu(s)} \left [ \exp (2 y^T A(s) y) \; A(s) y \right ]
  \\  & = \mathbf{E}_{s \sim \mu(s), s' \sim p(s'|s)} \left [ \exp (2 y^T A(s) y) \; \phi(s)(\phi(s) - \phi(s))^T y \right ]
  \intertext{We substitute $g(s)$ into this to arrive at the final gradient derivation}
   & = \mathbf{E}_{s \sim \mu(s), s' \sim p(s'|s)} \left [ \exp(2 g(s)) \; \phi(s)(\phi(s) - \phi(s'))^T y \right ].
\end{align}

This corresponds to the update rules:
\begin{align}
  a      & \gets \phi(s)^\top y                                                    \\
  b      & \gets \phi(s')^\top y                                                   \\
  \theta & \gets \theta - \alpha_1 (g_\theta(s) - a(a-b))\nabla_\theta g_\theta(s) \\
  y      & \gets y - \alpha_2 \exp(2 g_\theta(s)) (a-b) \phi(s)
\end{align}
where $\theta$ is the parameterization of $g(s)$, and $\alpha_\circ$ are learning rates.

And finally, to complete this, we multiply each update of $w$ by $\exp(2 g(s))$ to resample it so it appears to come from the ``safe'' distribution, which completes the description of the algorithm!



\subsection{POP-Q-Learning}\label{sec:popq}\label{sec:pop-q-derivation}

Thus far, we have focused on Markov Reward Processes.
For RL problems, we need to extend this approach to Markov Decision Processes (MDPs).
% We now extend this from simple Markov chains to MDPs by incorporating actions into it.
An MDP is a tuple, $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, with state space $\mathcal S$, transition function $P : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}_+$, reward function $R : \mathcal S \times \mathcal{A} \to \mathbb R$, and discount factor $\gamma \in [0, 1]$.
The goal in this setting is to find a probabilistic policy $\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}_+$ that maximizes the future discounted reward:
\begin{equation}
  \pi^\star = \argmax_\pi \E_\pi \left [ \sum_{t=0}^\infty \gamma^t R(s_t, a_t) \right ]
\end{equation}
Many RL methods use variations of Q-learning \cite{watkins1992q,mnih2015humanlevel,haarnoja2018soft,kumar2020cql}, which involves learning a state-action value function, or $Q$-function:
\begin{equation}
  Q^\pi(s, a) = \E_\pi \left[\left.\sum_{t=0}^\infty \gamma^t R(s_t, a_t) \right| s_0 = s, a_0 = a \right]
\end{equation}

By considering a fixed policy $\pi$, a combined state-space $\mathcal{X} = \mathcal{S} \times \mathcal{A}$, and a policy-conditioned transition function $\tilde{P}^\pi((s, a), (s', a')) = P(s, a, s') \pi(s', a')$, any MDP reduces to a Markov Chain.
Thus, as long as the NEC is satisfied in this modified state-space, we can bound the approximation error of the Q-function.
See \cref{sec:pop-q-derivation} for a detailed derivation.

Finally, for our method to applied to modern deep RL problems, we must extend our approach to non-linear Q-functions.
To do so, we approximate the Q-function with a neural network, $Q_{\theta^Q}$ parameterized by $\theta^Q$ and consider a stochastic parameterized policy $\pi_{\theta^\pi}$.
To update $Q_{\theta^Q}$, we used a squared Bellman loss, $\mathcal{L}_Q(\theta^Q) = (Q_{\theta^Q}(s, a) - r - \gamma Q_{\theta^Q}(s', \pi_{\theta^\pi}(s')))^2$, which we re-weight with our g-function as before.
For our offline RL experiments, we also add CQL regularization \citep{kumar2020cql} to our Q-learning updates to prevent over-optimism on low-support regions of the state-action space.
To update our linear dual variables $y$, we use the penultimate layer of $Q_{\theta^Q}$ as our feature vector.
Finally, we use a SAC-style entropy regularized loss to update our policy network, $\pi_{\theta^\pi}$.
\Cref{alg:example} provides an overview of our method.


\begin{algorithm}[tb]
  \caption{Deep POP-Q-Learning}
  \label{alg:example}
  \begin{algorithmic}
    \STATE  Initialize Q-function, $Q_{\theta^Q}$, g-function, $g_{\theta^g}$, dual variable vector $y$, and some policy $\pi_{\theta^\pi}$.
    \FOR{step $t$ in ${1, \ldots, N}$}
    \STATE Sample mini-batch $(s, a, r, s') \sim \mu$.
    \STATE Sample $\tilde{a} \sim \pi_{\theta^\pi}(s), \tilde{a}' \sim \pi_{\theta^\pi}(s')$.
    \STATE \# Compute features from penultimate layer of Q-network:
    \STATE $\phi \gets Q_{\theta^Q}(s, a), \phi' \gets Q_{\theta^Q}(s', \tilde{a}')$.
    \STATE \# Update g-function and dual variable vectors:
    \STATE $\theta^g_t \gets \theta^g_{t+1} - \eta_g \nabla_{\theta^g} [g_{\theta^g}(s, a) - (\phi^\top y) (\phi - \phi')^\top y]^2$
    \STATE $y_t \gets y_{t+1} - \eta_y \exp (2 g_{\theta^g}(s, a)) [(\phi - \phi)^\top y \cdot \phi]$
    \STATE \# Update Q-function using re-weighted Q-loss update:
    \STATE $\theta^Q_t \gets \theta^Q_{t+1} - \eta_Q \exp (2 g_{\theta^g}(s, a)) \nabla_{\theta^Q} \mathcal{L}_Q(\theta^Q)$
    \STATE \# Update policy with SAC-style loss:
    \STATE $\theta^\pi_t \gets \theta^\pi_{t+1} - \eta_\pi \nabla_{\theta_\pi} [Q_{\theta^Q}(s, \tilde{a}) - \log \pi_{\theta^\pi}(\tilde{a} | s)]$
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\section{Experiments and Discussion}

We first apply POP-TD to a well-understood example so that we can directly illustrate the how it resamples TD updates to a ``safe'' distribution. We use the simple three-state task from \cref{fig:threestate}, including the specified transition function, value function, and basis. Since this is a policy evaluation task, there is no policy to be separately learned.

For illustration purposes, we select the family of distributions $\pi = (\sfrac h 2, \sfrac h 2, 1- h)$ parameterized by $h \in [0, 1]$.
This characterizes the possible distributions of data that we will present to POP-TD and naive TD in this experiment. The on-policy distribution corresponds to $h_o\approx 0.51$, and divides the family of distributions into a left subset ($h \leq h_o$) where KNEC holds and a right subset ($h_o > 0.5$) where KNEC does not. This is immediately apparent in \cref{fig:threestate}, where we plot the error at convergence from running naive- and POP-TD above, and the effective distribution of TD updates after reweighing below.
In the left subset, where KNEC holds, POP-TD does not resample TD updates at all. Therefore, the error of POP-TD tracks naive TD (top), and the effective distribution of TD updates in POP-TD and naive TD are the same as the data distribution (bottom).

In the right subset, we observe that naive TD converges to poor solutions with large error while POP-TD is able to learn with low error.
Directly computing the effective distribution, we see that naive TD adheres to the data distribution but POP-TD resamples the TD updates.
Looking at the behavior of POP-TD in the right subset, we see that POP-TD resamples updates to the on-policy distribution $p_o$ in $p\in[p_o,0.9]$, corresponding to the horizontal segment. This allows the learned Q-function to have very low error in that domain. As the data distribution becomes more extreme ($p\in [0.9, 1)$), POP-TD is not quite able to learn the resampling ratio, and so the effective distribution shifts away from $p_o$. This leads to a corresponding slight increase in error at extreme ratios. From this we observe that POP-TD requires full support of the sampling distribution, similar to many offline RL algorithms \cite{kumar2020cql,shi2022pessimistic}.

This simple experiment cleanly illustrates how POP-TD resamples TD updates to come from a ``safe'' distribution, and how that can greatly reduce the error in a policy evaluation task.

\label{sec:threestateexp}
\begin{figure}[t]
  \centering
  \input{poptd/threestate/threestatedistrib.tex}
  \caption{The error in the learned value function by naive- and POP-TD, plotted against a varying sampling distribution. In the left half of the plot, KNEC holds, and so POP-TD tracks the error of naive TD closely. In the right half of the plot naive TD diverges, while POP-TD resamples the data to a ``safe'' distribution and does not diverge. }
  \label{fig:threestate}
\end{figure}



\subsection{Tabular POP-Q on GridWorld}

\label{sec:expsimpleoffpolicy}
\begin{figure}[t]
  \centering
  \input{poptd/simpleoffpolicy/errors_plot.tex}
  \caption{Q-function errors for naive and POP Q-Learning on \cref{fig:sopmap}, over 50 randomly sampled bases. Points below the line correspond to bases where POP-Q has lower error. POP-Q substantially reduces error at convergence in about half the sampled bases. }
  \label{fig:soperr}
\end{figure}

In this experiment, we consider the the simple grid environment from \cref{fig:sopmap}.
Our training data is sampled following the suboptimal data policy ({\color{datapolicy} \datapolicyglyph}), adding uniform random dithering to guarantee that every state-action pair is represented.
Our goal is to approximate the true Q-function, eventually recovering the optimal policy ({\color{optimpolicy} \optimpolicyglyph}).
We represent the Q-function as a linear function with a fixed random basis, training it to convergence using naive Q-learning and POP-Q separately. (For POP-Q, we also randomly initialize a tabular $y$ and $g$ separately.) We also compute the ground-truth Q-function using tabular Q-learning, which must converge to the global optimum \cite{watkins1992q}.

We plot the error at convergence over 50 different random initializations in \cref{fig:soperr}, where the horizontal position of the point indicates the error when Naive TD is used, and the vertical position indicated the error with POP-TD. About half of the points are below the diagonal line, indicating that POP-Q has lower error in those cases. For comparison, we annotate the mean error if similarly-initialized Q-functions were trained following the optimal policy (\emph{On-Policy}), which represents the best-case error in expectation.

The distribution of errors in naive Q-learning spans many orders of magnitude. This is expected because we have deliberately engineered the task to be unstable. The stability of TD in each case depends on the basis, which is randomly sampled. By comparing the performance between POP-TD and naive TD on the with the same basis, we can show that POP-TD is able to mitigate instability on bases where naive TD is unstable.

POP-Q attempts to resample updates to find a distribution where KNEC holds. We can indirectly observe this tendency by finding the most negative eigenvalue of the A-matrix (from \cref{eqn:amatrix}), and comparing it before and after reweighing. In 90\% of the cases, the smallest eigenvalue increases after reweighing, suggesting that POP-Q is successfully resampling the data to a distribution closer to one where KNEC holds.


\subsection{Linear POP-Q on GridWorld}

\label{sec:explinearoffpolicy}
\begin{figure}[t]
  \centering
  \input{poptd/simpleoffpolicy/errors_plot.tex}
  \caption{Q-function errors for naive and POP Q-Learning on \cref{fig:sopmap}, over 50 randomly sampled bases. Points below the line correspond to bases where POP-Q has lower error. POP-Q substantially reduces error at convergence in about half the sampled bases. }
  \label{fig:linearerr}
\end{figure}

This is a similar experiment to that in \cref{sec:expsimpleoffpolicy}, but with linear approximation. One key step on the road to getting POP-Q to work on larger experiments is to understand the behavior of POP-Q under function approximation. Function approximation is necessary because, in the tabular case, POP-Q uses as much memory as a tabular Q-learning algorithm would; this is intractable for most practical problems. We also wish to exploit the generalization afforded to us by neural networks, to hopefully learn more accurate models with less data.

In this experiment, we define $g_\phi : \mathcal S \to \mathbb R$ as:
\begin{align}
  g_\phi & = \Phi_g \theta^g
\end{align}
for basis $\Phi_g \in \mathbb R^{n\times m}$ and learned weights $\theta^g \in \mathbb R^{m}$. We are given some random $\Phi_g$ and wish to learn $\theta^g$ so that $g_\phi$ acts as the reweighing function of POP-TD.

We conduct a series of experiments to understand the relationship between the degree of function approximation in $g$ and the quality of the learned model. Our example is engineered so that the ratio of two specific transitions (where the two trajectories initially diverge) most determines stability.

When performing experiments, we note that the performance of POP-TD depends sharply on the condition number of $\Phi$, but not necessarily that of $\Phi_g$. Specifically, we see that an orthogonal initialization step on $\Phi$ is crucial for performance. (In this step we set $\Phi$ to the orthogonal matrix of the QR-decomposition of a matrix where entries are sampled uniformly at random.)
We conjecture that this happens because POP-TD seeks to stochastically learn $\Phi^T A \Phi$, and a poor condition number of $\Phi$ leads to values that span multiple orders of magnitude and linear approximation is known to perform poorly on such data.

%\\ TODO: Check for $\Phi_g$  

The divergence appears to happen when the bases are selected so that this


\section{Ongoing Work}

While preliminary results are promising, we need to adapt this to standard RL tasks to showcase its effectiveness.

\subsection{POP-Q with linear G on GridWorld}

The current experiments with POP-Q learning all use a tabular g. This works, but takes the same memory as would learning a tabular value function, which would provably converge to the global optimum (side-stepping the entire problem).

Currently, we are evaluating POP-Q with approximate $g$ on our GridWorld example. We are running an ablation study to understand the relationship between the degree of approximation of $g$ and the performance of the resultant system.

\subsection{D4RL Random Datasets}

\begin{figure}[t]
  \centering
  \input{poptd/d4rl/halfcheetah.tex}
  \caption{
    POP-Q learning performs slightly but significantly better than naive Q learning on this run of the D4RL dataset \texttt{halfcheetah-random-v2}. The plot is smoothed, and shaded sections represent the standard error over 8 seeds. POP-Q may help us deal with datasets generated by vastly different policies.}
  \label{fig:d4rl}
\end{figure}

Offline RL is a perfect application for our method.
As discussed earlier, POP-Q reweighing specifically addresses \textit{proportional mismatch} as opposed to \textit{support mismatch}.
For this reason, we combine our method with the state-of-the-art method for addressing support mismatch in offline RL settings.
Specifically, we augment Conservative Q-Learning (CQL) \citep{kumar2020cql} with our projected off-policy technique.
The resultant algorithm, called POP-CQL, parameterizes the Q-function and $g_\theta(s, a)$ as separate neural networks. We still learn a linear $y$, treating the output of the penultimate layer of the Q-function as the basis.
CQL works by penalizing out-of-distribution actions to ensure the learned policy remains within the support of the data, but it does not directly constrain the distribution over states implied by the policy. The use of POP methods in addition to CQL allows us to mitigate the remaining distribution shift, the proportional mismatch.

We also need to check if our method is better at addressing proportional mismatch than importance sampling. Therefore, we also wish to compare against CQL augmented with DualDICE \citep{nachum2019dualdice}. DualDICE is an importance sampling method that aims to mitigate the challenges of high-variance updates on the importance sampling weights.




\subsection{Mixed-Policy Datasets}

Finally, our POP technique allows us to learn when multiple policies are mixed, even when part of the dataset comes from a potentially adversarial policy. This is especially relevant if we wish to learn from expert-produced data that comes from different experts, or if the dataset may include samples from adversaries.

To illustrate this, we generate a custom dataset in the style of D4RL. Using the \texttt{HalfCheetah} environment from MuJoCo, we combine $60,000$ samples generated by an adversarial policy that runs backwards (achieving $\approx13000$ score per episode) with $140,000$ samples generated by an expert policy (achieving about $11000$ score per episode). The net dataset achieves about 900 score per episode.

We train both CQL and POP-CQL on the resultant dataset and plot the performance in \cref{fig:fwdback}, which shows that POP-CQL performs better on average than CQL in this situation. While the performance of POP-CQL is stable throughout training, the best score ever achieved is by vanilla CQL early in the training process. We need to investigate this further to understand why CQL transiently performs better and see if we can further improve the performance of POP-CQL.

\begin{figure}[t]
  \centering
  %\input{poptd/fwdback/fwdback.tex}
  \includegraphics[width=\textwidth]{poptd/fwdback/mixedexpert.png}
  \caption{
    On custom mixed-policy dataset, POP-CQL learning has more consistent performance than naive Q learning, and is better at the limit of training. However, the highest performance achieved is by regular CQL, not POP-CQL. POP-Q can help us deal with datasets generated by vastly different policies, but more work needs to be done.
  }
  \label{fig:fwdback}
\end{figure}

\section{Conclusion}
In this chapter, we introduced a method for decreasing the approximation error off-policy for both state value functions, POP-TD, and state-action value functions, POP-Q. Under mild assumptions, our method guarantees that the off-policy approximation error for both state and state-action value functions is bounded. This is bourne out by toy and small-scale experiments, in which our method significantly decreases the approximation error for certain sampling functions.

Currently, we are considering the relationship between the degree of approximation of $g$ and the resultant performance of the method. Also, to establish the practicality of the method, can integrate our method with Conservative Q-Learning and test the resultant POP-CQL algorithm on a set of standard D4RL locomotion tasks. We are currently working on evaluating the performance of this combination method and understanding how to improve its performance.
